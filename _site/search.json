[
  {
    "objectID": "sections/solution.html",
    "href": "sections/solution.html",
    "title": "Solution",
    "section": "",
    "text": "In the following sections we describe our recommendations for: * Increasing semantic interoperability between tools in the EconDataverse * Linking data to other data and metadata * Constraining data and metadata using ontologies * Developing new tools that rely on semantic interoperability, linked data, and ontologies to streamline economic data reuse\n\n\nThe EconDataverse is both a set of software tools in R and Python and a set of conventions and best practices for developing tools that work with economic data. There are coding conventions that ensure consistent style across packages as well as data syntax conventions that ensure compatibility with popular data manipulation and visualization libraries in both R and Python.\nInteroperability between tools in the EconDataverse is a key concern. Software tools that do not interoperate create additional toil for the user, contrary to our stated goal of eliminating toil. In the EconDataverse the problem of interoperability is one of ensuring the tools “speak the same language”. A simple approach to this problem is to consider language as equivalent to syntax, and ensure that tools consume and produce the same syntax, such as the tidy CSV (TKTK ref) currently used by tools in the EconDataverse. This reduces the toil associated with transforming syntax between tools but leaves the problem of semantic interoperability.\nIn this context semantic interoperability is the ability of different tools in the EconDataverse to exchange data with unambiguous, shared meaning. It ensures that the meaning of the data is preserved and correctly interpreted between tools, even if those tools use different technologies, such as R and Python.\nSemantic interoperability starts from agreeing on unambiguous identifiers for things. For example, the EconDataverse package maintainers have agreed to refer to countries using ISO 3166-1 standard codes and to represent years using four digits and the Gregorian calendar. This consensus ensures that data from different sources about the same countries and years can be joined, as in the R example below:\n\n# Load required package\nlibrary(dplyr)\n\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# --- Perform inner join on ISO country code and year ---\njoined_data &lt;- inner_join(macro_data, population_data, by = c(\"iso3c\", \"year\"))\nImagine instead that different datasets used different identifiers for countries: one dataset referred to the United States as “États-Unis” while the other referred to it as “USA”:\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"États-Unis\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# Join on USA data is not going to work as intended here.\nThe data analyst would have to first resolve the discrepancy – mapping “États-Unis” to “USA” or vice versa – in order to harmonize data from multiple sources. That work is toil that can be eliminated by agreeing on unambiguous identifiers and using them consistently.\n\n\nThe R example in figure X also includes numeric data such as GDP (in billions of of US Dollars) and population (in millions). Unlike the ISO 3166 country codes, these data are ambiguous. A human data analyst can infer at least part of the meaning from the column name (gdp_billion) and the accompanying comment (“GDP (in USD billions)”). Other parts are still ambiguous – is it nominal or real GDP?\nWe would like to eliminate this ambiguity and the ensuing toil by associating numeric data with unambiguous metadata. The SDMX Information Model recommends that the metadata for a numeric datum include:\n\nDimensions whose combined values uniquely identify that datum in a cube/hypercube of dimensions\nAttributes of the datum, such as units of measurement\n\nThe dimensions uniquely identifying a single nominal GDP measurement are:\n\nPlace: the country associated with the GDP (e.g., “USA” and “FRA”)\nTime: the year the GDP was measured (e.g., 2024).\nIndicator: nominal GDP\n\nDimensions like “place” and attributes like “unit of measurement” should themselves be uniquely identified, and have associated properties such as human-readable labels and definitions, relationships with other dimensions or attributes, and constraints on the values that can be associated with a dimension (e.g., code lists). The SDMX Content-Oriented Guidelines provide a library of well-known, uniquely-identified dimensions and attributes that can be reused in many different domains.\n\n\n\n\nWe can use the unambiguous identifier for a thing (“USA”) in the data as a key to resolve additional properties about a thing as well as its relationships with other things following the Linked Data principles. For example, we can use schema.org vocabularies to describe the relationship between the country of the United States and the continent of North America, in a machine-readable format (an RDF knowledge graph represented in Turtle):\n@prefix ex: &lt;http://example.org/entities/&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\n# United States of America\nex:USA a schema:Country ;\n    # Human-readable labels in multiple human languages\n    schema:name \"United States of America\"@en ;\n    schema:name \"États-Unis\"@fr ;\n    schema:identifier \"USA\" ;\n    # A relationship: the country United States is contained in the place North America.\n    schema:containedInPlace ex:NA .\n\n# North America\nex:NA a schema:Continent ;\n    schema:name \"North America\"@en ;\n    schema:identifier \"NA\" .\n\n\n\nThere is an additional level of consensus at work in the example above. By referencing schema.org we are implicitly agreeing that the unambiguous identifier schema:containedInPlace refers to a way of relating two places. That identifier can be used in turn as a key to further (linked) metadata:\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\nschema:containedInPlace a rdf:Property ;\n    rdfs:label \"containedInPlace\" ;\n    rdfs:comment \"The basic containment relation between a place and one that contains it.\" ;\n    schema:domainIncludes schema:Place ;\n    schema:inverseOf schema:containsPlace ;\n    schema:rangeIncludes schema:Place .\nThis metadata is part of an ontology. An ontology is an “explicit specification of a conceptualization” (TKTK ref Gruber) of a domain, including the “the types, properties, and interrelationships of entities that exist for a particular domain of discourse” (TKTK ref Gene Ontology Consortium). An ontology includes machine-readable specifications as well as human-readable labels and definitions that codify shared understanding of a domain. In the snippet above the domain is geography – places and a valid relationship (contained-in) between them. The ontology is not about specific places, but about the category “place”, its possible properties, and how it relates to other categories. The example eliminates ambiguity what schema:containedIn means through the use of a human-readable definition (“The basic containment relation …”) as well as machine-enforceable constraints on the categories of things that can be involved in the schema:containedInPlace relation (the schema:domainIncludes and schema:rangeIncludes). By constraining the domain of discourse an ontology amplifies the benefits of reusing identifiers by providing further clarity to humans as well as a way for machines to ensure that data and metadata stay within those constraints.\n\n\nOntologies like the Financial Industry Business Ontology are commonly used in the financial services sector to enable cross-system federation and aggregation of data in order to support decision-making, streamline regulatory reporting, and encourage the adoption of advanced analytical capabilities.\nUnfortunately, the macroeconomics domain does not have an ontology comparable in scope and rigour to FIBO. We recommend developing an ontology of macroeconomics incrementally. At a minimum this ontology should model:\n\nthe structure of statistical data cubes (dimensions, attributes, measures)\ncommon dimensions such as time, place, and macroeconomic indicators\ncommon attributes such as units\n\nIn the long term the ontology should also model:\n\nrelationships between dimensions e.g., different macroeconomic indicators\nrelationships between attributes e.g., unit conversions\ndata lineage and provenance\nmodeling assumptions\ninformation about economic organizations, people, and the relationships between them (a social network)\ninformation about places and their relationships (e.g., aggregates such as Low Income Countries)\nmachine-readable assertions extracted from human-written web pages and other documents (a knowledge graph about the economic world)\n\nFortunately, there are a number of existing standards that can serve as building blocks for the ontology, including:\n\nSDMX Data Structure Definitions (DSDs) e.g., Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)\nthe RDF Data Cube Vocabulary for publishing multi-dimensional statistical data as Linked Data\nthe PROV Ontology of data lineage and provenance\nQUDT ontologies of units of measure, quantity kind, dimensions and data types\nSKOS specifications and standards for knowledge organization systems (KOS) (thesauri, taxonomies, et al.)\nthe VoID vocabulary for describing linked datasets\nSchema.org cross-domain vocabularies\n\n\n\n\n\nIncreasing semantic interoperability, developing ontologies, and taking advantage of linked data will reduce the toil and increase the productivity of using existing tools in the EconDataverse. However, in order to fully address the challenges highlighted in TKTK section ref, we also recommend expanding and refining the EconDataverse toolkit in three key areas:\n\ntools for extracting and transforming economic data and metadata\ntools for exploring and finding economic data and metadata\ntools for analyzing and manipulating financial models\n\nNew tools in these areas should produce and consume data and metadata that conform to the ontology in order to enable new features, like providing unified views over heterogeneous data sources.\nFor the short- to mid-term we recommend implementing an adapter library and command-line program that transforms ontology-conformant data to and from the formats expected by existing EconDataverse tools. In the long term these tools should be retrofitted to produce and consume ontology-conformant data and metadata.\n\n\nThe majority of packages in the current EconDataverse package ecosystem extract data from IMF, World Bank, and other sources and transform them into tidy data frames, with each variable as a column and each observation as a row. Metadata are typically limited to column names, which are not explicitly grounded in an ontology that would provide context and definitions.\nGoing forward, these packages should produce ontology-conformant data and metadata that fully capture the semantics of the source data. These can be easily but lossily transformed into convenient but semantics-poor formats like tidy data frames for compatibility with third party tools.\nExtraction should not be limited to tabular data sources. The ontology can also be used to guide Large Language Models (LLMs) in the extraction of ontology-conformant structured data from natural language sources such as the World Bank country and climate development reports.\n\n\n\nTools for exploring and finding data and metadata tend to be useful in proportion to how closely the data’s representation corresponds to the domain abstractions, like accounts and sub-accounts in Balance of Payments datasets. This requires a level of abstraction and interpretation that is difficult to achieve without building data source-specific tools. Instead, most generic tools work with data and metadata at the lowest common denominator of abstraction – as tables and rows of figures to be scanned or a corpus of arbitrary text to be searched. The problem of interpretation is mostly left up to the user.\nHaving data and metadata from multiple sources conform to a rich, domain-specific ontology makes it possible to build tools that work at a higher level of abstraction without sacrificing source independence. Tools can make assumptions about the meaning of the data and relationships between them, which dramatically increases the tools’ power. For example, the next generation of EconDataverse tools could:\n\ngroup or aggregate data from different (geographic) reference areas by considering explicit relationships between areas, such as part-whole or collection membership\nlet users browse and filter cube-structured datasets and slices by the dimensions they incorporate (e.g., time, place, gender), provenance, or recency\nsuggest inputs for a given model by matching metadata about the inputs with metadata from a data/metadata catalog\n\n\n\n\nFinally, the EconDataverse needs tools for analyzing and financial models, which are typically encoded in Excel spreadsheets. In the short term, tools should automate or semi-automate toilsome tasks like:\n\nManually mapping statistical data cube measures to spreadsheet inputs\nManually reverse-engineering spreadsheets to identify which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs\nManually permuting spreadsheets inputs in order to perform sensitivity analyses\n\n\n\n\nMuch of the toil in using financial models lies in the process of identifying the right source data to supply as model inputs. For example, the Debt Dynamics Toolkit spreadsheet requires the following inputs in a single sheet:\n\nDynamic Debt Toolkit inputs sheet\n\n\n\n\n\n\n\nYear / Variable\n2011\n2012\n\n\n\n\ndt (debt including uncalled guarantees): stock of total gross public debt, percent of GDP\n31.59\n35.34\n\n\no/w stock of local-currency guarantees (uncalled): stock of uncalled guarantees in local currency included in total debt, percent of GDP\n0.00\n0.00\n\n\no/w stock of foreign-currency guarantees (uncalled): stock of uncalled guarantees in foreign currency included in total debt, percent of GDP\n0.00\n0.00\n\n\nαt (share excl. guarantees): share of foreign currency denominated debt in total debt, percent of total debt\n55.05\n58.87\n\n\net (LCU/FCU, avg): nominal average exchange rate, local currency per unit of foreign currency\n18.92\n19.50\n\n\net (LCU/FCU, eop): nominal end of period exchange rate, local currency per unit of foreign currency\n19.05\n19.96\n\n\nitd: nominal effective interest rate on local currency denominated debt, percent\n8.92\n9.23\n\n\nitf: nominal effective interest rate on foreign currency denominated debt, percent\n2.46\n2.03\n\n\nπt: GDP deflator inflation, percent\n5.60\n5.39\n\n\ngt: Real GDP growth, percent\n3.84\n4.13\n\n\npbt: Primary balance, percent of GDP\n-3.28\n-4.03\n\n\noft (other net debt-creating flows): Other net debt creating flows, percent of GDP\n0.00\n0.00\n\n\nπft: Foreign GDP deflator inflation, percent (used in fan chart)\n2.09\n1.92\n\n\n\nPer the data cube model, each input cell can be uniquely identified by dimensions:\n\nTime: with controlled values 2011 and 2012\nPlace: implicitly, the country whose debt is being analyzed\nIndicator: such as “Real GDP growth”\n\nIn this case the year and place are unambiguously identified and are relatively easy to match to source data. It is not as obvious how to map the indicator “nominal effective interest rate on foreign currency denominated debt, percent” to a specific source database.\nOne approach is to treat “nominal effective interest rate on foreign currency denominated debt, percent” as a unique identifier as-is, like Google Data Commons does. This assumes that the source data sources are also using this identifier, which is unlikeliy.\nThe approach we recommend is to decompose the name into a structured combination of sub-identifiers:\n\na base indicator: “interest rate”\nqualifiers on the base indicator: “nominal”, “effective”, “on foreign currency denominated debt”\nattributes: “percent”, which should be treated as an attribute of the data rather than as part of the indicator dimension\n\nThe structure and valid combinations of indicator, qualifier, and attribute values should be dictated by the ontology. Given a permissible set of controlled values, a Large Language Model could aid in this decomposition from natural language.\nThis approach parallels the way World Bank debt codes can be decomposed into segments. For example, DT.DOD.[Debt Instrument].[Debtor/Creditor Sector].[Unit] can be decomposed into:\n\nDT: Debt\nDOD: Debt Outstanding and Disbursed\nInstrument Type (e.g., long-term, short-term, use of IMF credit)\nDebtor or Creditor (e.g., public, private, multilateral, bilateral)\nUnit of measure (e.g., CD = current US dollars, GD.ZS = % of GNI)\n\n\n\n\nIn the long term the financial models themselves should be treated as data, and extracted and transformed into ontology-conformant data and metadata. From there they could be lossily transformed back into spreadsheets. This approach would build on open source precedents like Morphir, a system that captures an application’s domain model and business logic in a technology agnostic manner."
  },
  {
    "objectID": "sections/solution.html#semantic-interoperability",
    "href": "sections/solution.html#semantic-interoperability",
    "title": "Solution",
    "section": "",
    "text": "The EconDataverse is both a set of software tools in R and Python and a set of conventions and best practices for developing tools that work with economic data. There are coding conventions that ensure consistent style across packages as well as data syntax conventions that ensure compatibility with popular data manipulation and visualization libraries in both R and Python.\nInteroperability between tools in the EconDataverse is a key concern. Software tools that do not interoperate create additional toil for the user, contrary to our stated goal of eliminating toil. In the EconDataverse the problem of interoperability is one of ensuring the tools “speak the same language”. A simple approach to this problem is to consider language as equivalent to syntax, and ensure that tools consume and produce the same syntax, such as the tidy CSV (TKTK ref) currently used by tools in the EconDataverse. This reduces the toil associated with transforming syntax between tools but leaves the problem of semantic interoperability.\nIn this context semantic interoperability is the ability of different tools in the EconDataverse to exchange data with unambiguous, shared meaning. It ensures that the meaning of the data is preserved and correctly interpreted between tools, even if those tools use different technologies, such as R and Python.\nSemantic interoperability starts from agreeing on unambiguous identifiers for things. For example, the EconDataverse package maintainers have agreed to refer to countries using ISO 3166-1 standard codes and to represent years using four digits and the Gregorian calendar. This consensus ensures that data from different sources about the same countries and years can be joined, as in the R example below:\n\n# Load required package\nlibrary(dplyr)\n\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# --- Perform inner join on ISO country code and year ---\njoined_data &lt;- inner_join(macro_data, population_data, by = c(\"iso3c\", \"year\"))\nImagine instead that different datasets used different identifiers for countries: one dataset referred to the United States as “États-Unis” while the other referred to it as “USA”:\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"États-Unis\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# Join on USA data is not going to work as intended here.\nThe data analyst would have to first resolve the discrepancy – mapping “États-Unis” to “USA” or vice versa – in order to harmonize data from multiple sources. That work is toil that can be eliminated by agreeing on unambiguous identifiers and using them consistently.\n\n\nThe R example in figure X also includes numeric data such as GDP (in billions of of US Dollars) and population (in millions). Unlike the ISO 3166 country codes, these data are ambiguous. A human data analyst can infer at least part of the meaning from the column name (gdp_billion) and the accompanying comment (“GDP (in USD billions)”). Other parts are still ambiguous – is it nominal or real GDP?\nWe would like to eliminate this ambiguity and the ensuing toil by associating numeric data with unambiguous metadata. The SDMX Information Model recommends that the metadata for a numeric datum include:\n\nDimensions whose combined values uniquely identify that datum in a cube/hypercube of dimensions\nAttributes of the datum, such as units of measurement\n\nThe dimensions uniquely identifying a single nominal GDP measurement are:\n\nPlace: the country associated with the GDP (e.g., “USA” and “FRA”)\nTime: the year the GDP was measured (e.g., 2024).\nIndicator: nominal GDP\n\nDimensions like “place” and attributes like “unit of measurement” should themselves be uniquely identified, and have associated properties such as human-readable labels and definitions, relationships with other dimensions or attributes, and constraints on the values that can be associated with a dimension (e.g., code lists). The SDMX Content-Oriented Guidelines provide a library of well-known, uniquely-identified dimensions and attributes that can be reused in many different domains."
  },
  {
    "objectID": "sections/solution.html#linking-data",
    "href": "sections/solution.html#linking-data",
    "title": "Solution",
    "section": "",
    "text": "We can use the unambiguous identifier for a thing (“USA”) in the data as a key to resolve additional properties about a thing as well as its relationships with other things following the Linked Data principles. For example, we can use schema.org vocabularies to describe the relationship between the country of the United States and the continent of North America, in a machine-readable format (an RDF knowledge graph represented in Turtle):\n@prefix ex: &lt;http://example.org/entities/&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\n# United States of America\nex:USA a schema:Country ;\n    # Human-readable labels in multiple human languages\n    schema:name \"United States of America\"@en ;\n    schema:name \"États-Unis\"@fr ;\n    schema:identifier \"USA\" ;\n    # A relationship: the country United States is contained in the place North America.\n    schema:containedInPlace ex:NA .\n\n# North America\nex:NA a schema:Continent ;\n    schema:name \"North America\"@en ;\n    schema:identifier \"NA\" ."
  },
  {
    "objectID": "sections/solution.html#constraining-data-and-metadata",
    "href": "sections/solution.html#constraining-data-and-metadata",
    "title": "Solution",
    "section": "",
    "text": "There is an additional level of consensus at work in the example above. By referencing schema.org we are implicitly agreeing that the unambiguous identifier schema:containedInPlace refers to a way of relating two places. That identifier can be used in turn as a key to further (linked) metadata:\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\nschema:containedInPlace a rdf:Property ;\n    rdfs:label \"containedInPlace\" ;\n    rdfs:comment \"The basic containment relation between a place and one that contains it.\" ;\n    schema:domainIncludes schema:Place ;\n    schema:inverseOf schema:containsPlace ;\n    schema:rangeIncludes schema:Place .\nThis metadata is part of an ontology. An ontology is an “explicit specification of a conceptualization” (TKTK ref Gruber) of a domain, including the “the types, properties, and interrelationships of entities that exist for a particular domain of discourse” (TKTK ref Gene Ontology Consortium). An ontology includes machine-readable specifications as well as human-readable labels and definitions that codify shared understanding of a domain. In the snippet above the domain is geography – places and a valid relationship (contained-in) between them. The ontology is not about specific places, but about the category “place”, its possible properties, and how it relates to other categories. The example eliminates ambiguity what schema:containedIn means through the use of a human-readable definition (“The basic containment relation …”) as well as machine-enforceable constraints on the categories of things that can be involved in the schema:containedInPlace relation (the schema:domainIncludes and schema:rangeIncludes). By constraining the domain of discourse an ontology amplifies the benefits of reusing identifiers by providing further clarity to humans as well as a way for machines to ensure that data and metadata stay within those constraints.\n\n\nOntologies like the Financial Industry Business Ontology are commonly used in the financial services sector to enable cross-system federation and aggregation of data in order to support decision-making, streamline regulatory reporting, and encourage the adoption of advanced analytical capabilities.\nUnfortunately, the macroeconomics domain does not have an ontology comparable in scope and rigour to FIBO. We recommend developing an ontology of macroeconomics incrementally. At a minimum this ontology should model:\n\nthe structure of statistical data cubes (dimensions, attributes, measures)\ncommon dimensions such as time, place, and macroeconomic indicators\ncommon attributes such as units\n\nIn the long term the ontology should also model:\n\nrelationships between dimensions e.g., different macroeconomic indicators\nrelationships between attributes e.g., unit conversions\ndata lineage and provenance\nmodeling assumptions\ninformation about economic organizations, people, and the relationships between them (a social network)\ninformation about places and their relationships (e.g., aggregates such as Low Income Countries)\nmachine-readable assertions extracted from human-written web pages and other documents (a knowledge graph about the economic world)\n\nFortunately, there are a number of existing standards that can serve as building blocks for the ontology, including:\n\nSDMX Data Structure Definitions (DSDs) e.g., Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)\nthe RDF Data Cube Vocabulary for publishing multi-dimensional statistical data as Linked Data\nthe PROV Ontology of data lineage and provenance\nQUDT ontologies of units of measure, quantity kind, dimensions and data types\nSKOS specifications and standards for knowledge organization systems (KOS) (thesauri, taxonomies, et al.)\nthe VoID vocabulary for describing linked datasets\nSchema.org cross-domain vocabularies"
  },
  {
    "objectID": "sections/solution.html#tools-to-streamline-economic-data-reuse",
    "href": "sections/solution.html#tools-to-streamline-economic-data-reuse",
    "title": "Solution",
    "section": "",
    "text": "Increasing semantic interoperability, developing ontologies, and taking advantage of linked data will reduce the toil and increase the productivity of using existing tools in the EconDataverse. However, in order to fully address the challenges highlighted in TKTK section ref, we also recommend expanding and refining the EconDataverse toolkit in three key areas:\n\ntools for extracting and transforming economic data and metadata\ntools for exploring and finding economic data and metadata\ntools for analyzing and manipulating financial models\n\nNew tools in these areas should produce and consume data and metadata that conform to the ontology in order to enable new features, like providing unified views over heterogeneous data sources.\nFor the short- to mid-term we recommend implementing an adapter library and command-line program that transforms ontology-conformant data to and from the formats expected by existing EconDataverse tools. In the long term these tools should be retrofitted to produce and consume ontology-conformant data and metadata.\n\n\nThe majority of packages in the current EconDataverse package ecosystem extract data from IMF, World Bank, and other sources and transform them into tidy data frames, with each variable as a column and each observation as a row. Metadata are typically limited to column names, which are not explicitly grounded in an ontology that would provide context and definitions.\nGoing forward, these packages should produce ontology-conformant data and metadata that fully capture the semantics of the source data. These can be easily but lossily transformed into convenient but semantics-poor formats like tidy data frames for compatibility with third party tools.\nExtraction should not be limited to tabular data sources. The ontology can also be used to guide Large Language Models (LLMs) in the extraction of ontology-conformant structured data from natural language sources such as the World Bank country and climate development reports.\n\n\n\nTools for exploring and finding data and metadata tend to be useful in proportion to how closely the data’s representation corresponds to the domain abstractions, like accounts and sub-accounts in Balance of Payments datasets. This requires a level of abstraction and interpretation that is difficult to achieve without building data source-specific tools. Instead, most generic tools work with data and metadata at the lowest common denominator of abstraction – as tables and rows of figures to be scanned or a corpus of arbitrary text to be searched. The problem of interpretation is mostly left up to the user.\nHaving data and metadata from multiple sources conform to a rich, domain-specific ontology makes it possible to build tools that work at a higher level of abstraction without sacrificing source independence. Tools can make assumptions about the meaning of the data and relationships between them, which dramatically increases the tools’ power. For example, the next generation of EconDataverse tools could:\n\ngroup or aggregate data from different (geographic) reference areas by considering explicit relationships between areas, such as part-whole or collection membership\nlet users browse and filter cube-structured datasets and slices by the dimensions they incorporate (e.g., time, place, gender), provenance, or recency\nsuggest inputs for a given model by matching metadata about the inputs with metadata from a data/metadata catalog\n\n\n\n\nFinally, the EconDataverse needs tools for analyzing and financial models, which are typically encoded in Excel spreadsheets. In the short term, tools should automate or semi-automate toilsome tasks like:\n\nManually mapping statistical data cube measures to spreadsheet inputs\nManually reverse-engineering spreadsheets to identify which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs\nManually permuting spreadsheets inputs in order to perform sensitivity analyses\n\n\n\n\nMuch of the toil in using financial models lies in the process of identifying the right source data to supply as model inputs. For example, the Debt Dynamics Toolkit spreadsheet requires the following inputs in a single sheet:\n\nDynamic Debt Toolkit inputs sheet\n\n\n\n\n\n\n\nYear / Variable\n2011\n2012\n\n\n\n\ndt (debt including uncalled guarantees): stock of total gross public debt, percent of GDP\n31.59\n35.34\n\n\no/w stock of local-currency guarantees (uncalled): stock of uncalled guarantees in local currency included in total debt, percent of GDP\n0.00\n0.00\n\n\no/w stock of foreign-currency guarantees (uncalled): stock of uncalled guarantees in foreign currency included in total debt, percent of GDP\n0.00\n0.00\n\n\nαt (share excl. guarantees): share of foreign currency denominated debt in total debt, percent of total debt\n55.05\n58.87\n\n\net (LCU/FCU, avg): nominal average exchange rate, local currency per unit of foreign currency\n18.92\n19.50\n\n\net (LCU/FCU, eop): nominal end of period exchange rate, local currency per unit of foreign currency\n19.05\n19.96\n\n\nitd: nominal effective interest rate on local currency denominated debt, percent\n8.92\n9.23\n\n\nitf: nominal effective interest rate on foreign currency denominated debt, percent\n2.46\n2.03\n\n\nπt: GDP deflator inflation, percent\n5.60\n5.39\n\n\ngt: Real GDP growth, percent\n3.84\n4.13\n\n\npbt: Primary balance, percent of GDP\n-3.28\n-4.03\n\n\noft (other net debt-creating flows): Other net debt creating flows, percent of GDP\n0.00\n0.00\n\n\nπft: Foreign GDP deflator inflation, percent (used in fan chart)\n2.09\n1.92\n\n\n\nPer the data cube model, each input cell can be uniquely identified by dimensions:\n\nTime: with controlled values 2011 and 2012\nPlace: implicitly, the country whose debt is being analyzed\nIndicator: such as “Real GDP growth”\n\nIn this case the year and place are unambiguously identified and are relatively easy to match to source data. It is not as obvious how to map the indicator “nominal effective interest rate on foreign currency denominated debt, percent” to a specific source database.\nOne approach is to treat “nominal effective interest rate on foreign currency denominated debt, percent” as a unique identifier as-is, like Google Data Commons does. This assumes that the source data sources are also using this identifier, which is unlikeliy.\nThe approach we recommend is to decompose the name into a structured combination of sub-identifiers:\n\na base indicator: “interest rate”\nqualifiers on the base indicator: “nominal”, “effective”, “on foreign currency denominated debt”\nattributes: “percent”, which should be treated as an attribute of the data rather than as part of the indicator dimension\n\nThe structure and valid combinations of indicator, qualifier, and attribute values should be dictated by the ontology. Given a permissible set of controlled values, a Large Language Model could aid in this decomposition from natural language.\nThis approach parallels the way World Bank debt codes can be decomposed into segments. For example, DT.DOD.[Debt Instrument].[Debtor/Creditor Sector].[Unit] can be decomposed into:\n\nDT: Debt\nDOD: Debt Outstanding and Disbursed\nInstrument Type (e.g., long-term, short-term, use of IMF credit)\nDebtor or Creditor (e.g., public, private, multilateral, bilateral)\nUnit of measure (e.g., CD = current US dollars, GD.ZS = % of GNI)\n\n\n\n\nIn the long term the financial models themselves should be treated as data, and extracted and transformed into ontology-conformant data and metadata. From there they could be lossily transformed back into spreadsheets. This approach would build on open source precedents like Morphir, a system that captures an application’s domain model and business logic in a technology agnostic manner."
  },
  {
    "objectID": "sections/recommendations.html",
    "href": "sections/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend an incremental, iterative approach to implementing the solutions proposed in the previous section, following agile principles in order to continuously deliver value to EconDataverse users.\nIn the following sub-sections we recommend first steps toward implementing the processes and tools described in the last section.\n\n\nWe recommend immediately incorporating code quality tools into existing EconDataverse development workflows, such as mypy type checking, ruff linting, and black formatting for Python. The other code quality tools and practices described in the previous section should be gradually adopted.\n\n\n\nMissing and sparse documentation are a clear barrier to adopting EconDataverse tools. For the short term we recommend standardizing the structure and style of existing documentation (primarily READMEs and inline function/method documentation) and adopting the Vale documentation linter.\n\n\n\nWe recommend curating a minimum viable ontology of economics that is sufficient to losslessly capture data originating and metadata originating from key data sources:\n\nSDMX\nGoogle Data Commons\nthe World Bank’s World Development Indicators (WDI) API\nthe IMF’s World Economic Outlook (WEO) database\n\n\n\n\nWe recommend developing an adapter library in Python and a command line program that uses the library to transform data and metadata to and from an ontology-conformant representation and other formats:\n\nSDMX-CSV\nGoogle Data Commons\nRDF\nThe tidy tabular formats produced and consumed by existing EconDataverse tools (as a compatibility stop-gap)\n\n\n\n\nWe recommend implementing a new tool for extracting ontology-conformant data from a document corpus such as the IMF’s Debt Sustainability Analysis Low-Income Countries (DSA LIC) reports. The reports comprise a mixture of narrative text, rendered tables, and charts as PDFs. The first release of the tool should focus on extracting statistical data cube-compatible data (dimensions, attributes, measures) from tables in the documents. We recommend experimenting with different libraries and APIs such as pdfplumber and AWS Textract to identify a solution that maximizes key extraction metrics (precision, recall, F1 score).\n\n\n\nWe recommend designing and developing a full-stack web application for exploring and finding ontology-conformant data and metadata. The application should consist of:\n\na database aggregated from multiple sources via the aforementioned adapter library and extraction tool\na reusable library mediating database access\na graphical user interface that offers both full-text search and faceted search\n\nSearch functionality should be benchmarked against standard information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG).\nSubsequent application releases could include additional functionality, such as:\n\na conversational user interface\none or more programmatic interfaces (APIs), such as a SPARQL endpoint or an SDMX Registry\nsupport for querying multiple public and private databases, such as Google Data Commons\n\nGiven the relatively small volume of data and their public nature, we recommend considering a static site generation framework such as Next.js in order the costs (financial, labor) associated with hosting the application.\n\n\n\nWe recommend identifying or developing a tool that accepts a spreadsheet-based financial model such as the LIC DSF or the Dynamic Debt Toolkit and identifies which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs. There are two fundamental approaches to this problem:\n\nWhite box: build a dependency graph of spreadsheet formulas spanning the inputs and outputs in order to identify which inputs the desired outputs depend on. This approach is likely to produce false positives – flagging a set of inputs larger than the true minimum – because it is insensitive to the magnitude of input changes.\nBlack box: mutate individual inputs and combinations of inputs and record which outputs change. This approach may produce false negatives – flagging a set of inputs smaller than the true minimum – because of untested interactions between inputs.\n\nIdeally a tool will utilize both approaches to check each other."
  },
  {
    "objectID": "sections/recommendations.html#automated-code-quality-tools",
    "href": "sections/recommendations.html#automated-code-quality-tools",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend immediately incorporating code quality tools into existing EconDataverse development workflows, such as mypy type checking, ruff linting, and black formatting for Python. The other code quality tools and practices described in the previous section should be gradually adopted."
  },
  {
    "objectID": "sections/recommendations.html#documentation",
    "href": "sections/recommendations.html#documentation",
    "title": "Recommendations",
    "section": "",
    "text": "Missing and sparse documentation are a clear barrier to adopting EconDataverse tools. For the short term we recommend standardizing the structure and style of existing documentation (primarily READMEs and inline function/method documentation) and adopting the Vale documentation linter."
  },
  {
    "objectID": "sections/recommendations.html#ontology-of-economics",
    "href": "sections/recommendations.html#ontology-of-economics",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend curating a minimum viable ontology of economics that is sufficient to losslessly capture data originating and metadata originating from key data sources:\n\nSDMX\nGoogle Data Commons\nthe World Bank’s World Development Indicators (WDI) API\nthe IMF’s World Economic Outlook (WEO) database"
  },
  {
    "objectID": "sections/recommendations.html#adapter-library-and-command-line-program",
    "href": "sections/recommendations.html#adapter-library-and-command-line-program",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend developing an adapter library in Python and a command line program that uses the library to transform data and metadata to and from an ontology-conformant representation and other formats:\n\nSDMX-CSV\nGoogle Data Commons\nRDF\nThe tidy tabular formats produced and consumed by existing EconDataverse tools (as a compatibility stop-gap)"
  },
  {
    "objectID": "sections/recommendations.html#tool-for-extracting-ontology-conformant-data-from-documents",
    "href": "sections/recommendations.html#tool-for-extracting-ontology-conformant-data-from-documents",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend implementing a new tool for extracting ontology-conformant data from a document corpus such as the IMF’s Debt Sustainability Analysis Low-Income Countries (DSA LIC) reports. The reports comprise a mixture of narrative text, rendered tables, and charts as PDFs. The first release of the tool should focus on extracting statistical data cube-compatible data (dimensions, attributes, measures) from tables in the documents. We recommend experimenting with different libraries and APIs such as pdfplumber and AWS Textract to identify a solution that maximizes key extraction metrics (precision, recall, F1 score)."
  },
  {
    "objectID": "sections/recommendations.html#tool-for-exploring-and-finding-ontology-conformant-data-and-metadata",
    "href": "sections/recommendations.html#tool-for-exploring-and-finding-ontology-conformant-data-and-metadata",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend designing and developing a full-stack web application for exploring and finding ontology-conformant data and metadata. The application should consist of:\n\na database aggregated from multiple sources via the aforementioned adapter library and extraction tool\na reusable library mediating database access\na graphical user interface that offers both full-text search and faceted search\n\nSearch functionality should be benchmarked against standard information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG).\nSubsequent application releases could include additional functionality, such as:\n\na conversational user interface\none or more programmatic interfaces (APIs), such as a SPARQL endpoint or an SDMX Registry\nsupport for querying multiple public and private databases, such as Google Data Commons\n\nGiven the relatively small volume of data and their public nature, we recommend considering a static site generation framework such as Next.js in order the costs (financial, labor) associated with hosting the application."
  },
  {
    "objectID": "sections/recommendations.html#tool-for-identifying-minimal-spreadsheet-inputs",
    "href": "sections/recommendations.html#tool-for-identifying-minimal-spreadsheet-inputs",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend identifying or developing a tool that accepts a spreadsheet-based financial model such as the LIC DSF or the Dynamic Debt Toolkit and identifies which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs. There are two fundamental approaches to this problem:\n\nWhite box: build a dependency graph of spreadsheet formulas spanning the inputs and outputs in order to identify which inputs the desired outputs depend on. This approach is likely to produce false positives – flagging a set of inputs larger than the true minimum – because it is insensitive to the magnitude of input changes.\nBlack box: mutate individual inputs and combinations of inputs and record which outputs change. This approach may produce false negatives – flagging a set of inputs smaller than the true minimum – because of untested interactions between inputs.\n\nIdeally a tool will utilize both approaches to check each other."
  },
  {
    "objectID": "sections/introduction.html",
    "href": "sections/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nSovereign advisory is the specialized guidance provided to sovereign entities—particularly Ministries of Finance (MOF) and their Debt Management Offices (DMOs)—to strengthen the management of public debt, fiscal policy, and macroeconomic risks. The mission of a sovereign advisor is vital to improving debt sustainability, enhancing access to capital markets, and enabling governments to make informed, data-driven policy decisions that foster long-term economic stability and growth.\nModern technologies offer a powerful pathway forward for transforming sovereign advisory work. Artificial Intelligence (AI) technologies—especially large language models (LLMs)—alongside advanced data systems such as knowledge graphs and ontologies, present a significant opportunity to enhance the capabilities of sovereign analysts. The ability of LLMs to identify and extract information from both structured and unstructured data makes them valuable tools for uncovering insights from complex macroeconomic datasets and policy documents. Knowledge graphs and ontologies structure economic, legal, and financial data in ways that make it more interoperable, findable, and reusable. Together, these technologies can streamline labor-intensive workflows and support more consistent, timely, and data-driven decision-making.\nThis report explores how AI-powered platforms, when combined with robust data technologies, can unlock new levels of efficiency and effectiveness in sovereign advisory. It provides a detailed analysis of current technologies, workflows, and pain points within the sovereign debt management landscape, identifying key bottlenecks that hinder productivity and insight generation. Building on that analysis, it presents a roadmap for developing tools—along with the associated standards, conventions, and best practices—that will help eliminate toil and enhance the efficiency of both sovereign advisors and resource-constrained DMOs.\nFinally, the report offers strategic recommendations for the EconDataverse toolkit. These include long-term foundational technologies and high-level data and software architectures to guide its growth—supporting increased data volume, source heterogeneity, and stakeholder diversity, while maximizing its impact on data-driven sovereign advisory work."
  },
  {
    "objectID": "sections/conclusion.html",
    "href": "sections/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Conclusion"
  },
  {
    "objectID": "report.html#streamlining-data-reuse",
    "href": "report.html#streamlining-data-reuse",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Streamlining data reuse",
    "text": "Streamlining data reuse\n\nwith other EconDataverse tools in Python and R\nTidy Data Outputs All packages return data in a tidy format, with each variable as a column, each observation as a row, and each data type as a table. Defaulting to tidy data ensures compatibility with popular data manipulation and visualization libraries in both R and Python.\n\nWherever feasible, EconDataverse interfaces are designed to work across languages. Consistency across languages enables analysts and researchers to incorporate these tools into various projects, regardless of language preference, and fosters a collaborative, multi-lingual community.\nEvery package within the EconDataverse follows a standardized interface with predictable patterns. For instance, functions for data retrieval are named consistently: *_get functions retrieve data from a specific source, *_list functions enumerate available resources.\nFor R packages, the EconDataverse adopts the tidyverse coding style and enforces it through {lintr}. For Python, we use the same naming conventions, but respect style conventions layed out in PEP 8. These styles enhance the readability of code and align with the practices of users familiar with the corresponding language."
  },
  {
    "objectID": "report.html#other-solution-criteria",
    "href": "report.html#other-solution-criteria",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Other solution criteria",
    "text": "Other solution criteria"
  },
  {
    "objectID": "report.html#modularity",
    "href": "report.html#modularity",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Modularity",
    "text": "Modularity\nUnix philosophy: do one thing well No kitchen sinks"
  },
  {
    "objectID": "report.html#stability",
    "href": "report.html#stability",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Stability",
    "text": "Stability\nIterative development\nEach package is thoroughly tested through extensive unit tests, which ensure consistent functionality, protect against regressions, and help users trust that the packages will work as expected across different environments."
  },
  {
    "objectID": "report.html#flexibility",
    "href": "report.html#flexibility",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Flexibility",
    "text": "Flexibility\nAccommodate future data sources, …"
  },
  {
    "objectID": "report.html#scalability",
    "href": "report.html#scalability",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Scalability",
    "text": "Scalability\nIn terms of people / development as well as data volume"
  },
  {
    "objectID": "report.html#vendor-neutral",
    "href": "report.html#vendor-neutral",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Vendor-neutral",
    "text": "Vendor-neutral"
  },
  {
    "objectID": "report.html#transparency",
    "href": "report.html#transparency",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Transparency (?)",
    "text": "Transparency (?)\nAll packages are released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community."
  },
  {
    "objectID": "report.html#semantic-interoperability",
    "href": "report.html#semantic-interoperability",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Semantic interoperability",
    "text": "Semantic interoperability\nThe EconDataverse is both a set of software tools in R and Python and a set of conventions and best practices for developing tools that work with economic data. There are coding conventions that ensure consistent style across packages as well as data syntax conventions that ensure compatibility with popular data manipulation and visualization libraries in both R and Python.\nInteroperability between tools in the EconDataverse is a key concern. Software tools that do not interoperate create additional toil for the user, contrary to our stated goal of eliminating toil. In the EconDataverse the problem of interoperability is one of ensuring the tools “speak the same language”. A simple approach to this problem is to consider language as equivalent to syntax, and ensure that tools consume and produce the same syntax, such as the tidy CSV (TKTK ref) currently used by tools in the EconDataverse. This reduces the toil associated with transforming syntax between tools but leaves the problem of semantic interoperability.\nIn this context semantic interoperability is the ability of different tools in the EconDataverse to exchange data with unambiguous, shared meaning. It ensures that the meaning of the data is preserved and correctly interpreted between tools, even if those tools use different technologies, such as R and Python.\nSemantic interoperability starts from agreeing on unambiguous identifiers for things. For example, the EconDataverse package maintainers have agreed to refer to countries using ISO 3166-1 standard codes and to represent years using four digits and the Gregorian calendar. This consensus ensures that data from different sources about the same countries and years can be joined, as in the R example below:\n\n# Load required package\nlibrary(dplyr)\n\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# --- Perform inner join on ISO country code and year ---\njoined_data &lt;- inner_join(macro_data, population_data, by = c(\"iso3c\", \"year\"))\nImagine instead that different datasets used different identifiers for countries: one dataset referred to the United States as “États-Unis” while the other referred to it as “USA”:\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"États-Unis\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# Join on USA data is not going to work as intended here.\nThe data analyst would have to first resolve the discrepancy – mapping “États-Unis” to “USA” or vice versa – in order to harmonize data from multiple sources. That work is toil that can be eliminated by agreeing on unambiguous identifiers and using them consistently.\n\nUniquely identifying numeric data\nThe R example in figure X also includes numeric data such as GDP (in billions of of US Dollars) and population (in millions). Unlike the ISO 3166 country codes, these data are ambiguous. A human data analyst can infer at least part of the meaning from the column name (gdp_billion) and the accompanying comment (“GDP (in USD billions)”). Other parts are still ambiguous – is it nominal or real GDP?\nWe would like to eliminate this ambiguity and the ensuing toil by associating numeric data with unambiguous metadata. The SDMX Information Model recommends that the metadata for a numeric datum include:\n\nDimensions whose combined values uniquely identify that datum in a cube/hypercube of dimensions\nAttributes of the datum, such as units of measurement\n\nThe dimensions uniquely identifying a single nominal GDP measurement are:\n\nPlace: the country associated with the GDP (e.g., “USA” and “FRA”)\nTime: the year the GDP was measured (e.g., 2024).\nIndicator: nominal GDP\n\nDimensions like “place” and attributes like “unit of measurement” should themselves be uniquely identified, and have associated properties such as human-readable labels and definitions, relationships with other dimensions or attributes, and constraints on the values that can be associated with a dimension (e.g., code lists). The SDMX Content-Oriented Guidelines provide a library of well-known, uniquely-identified dimensions and attributes that can be reused in many different domains."
  },
  {
    "objectID": "report.html#linking-data",
    "href": "report.html#linking-data",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Linking data",
    "text": "Linking data\nWe can use the unambiguous identifier for a thing (“USA”) in the data as a key to resolve additional properties about a thing as well as its relationships with other things following the Linked Data principles. For example, we can use schema.org vocabularies to describe the relationship between the country of the United States and the continent of North America, in a machine-readable format (an RDF knowledge graph represented in Turtle):\n@prefix ex: &lt;http://example.org/entities/&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\n# United States of America\nex:USA a schema:Country ;\n    # Human-readable labels in multiple human languages\n    schema:name \"United States of America\"@en ;\n    schema:name \"États-Unis\"@fr ;\n    schema:identifier \"USA\" ;\n    # A relationship: the country United States is contained in the place North America.\n    schema:containedInPlace ex:NA .\n\n# North America\nex:NA a schema:Continent ;\n    schema:name \"North America\"@en ;\n    schema:identifier \"NA\" ."
  },
  {
    "objectID": "report.html#constraining-data-and-metadata",
    "href": "report.html#constraining-data-and-metadata",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Constraining data and metadata",
    "text": "Constraining data and metadata\nThere is an additional level of consensus at work in the example above. By referencing schema.org we are implicitly agreeing that the unambiguous identifier schema:containedInPlace refers to a way of relating two places. That identifier can be used in turn as a key to further (linked) metadata:\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\nschema:containedInPlace a rdf:Property ;\n    rdfs:label \"containedInPlace\" ;\n    rdfs:comment \"The basic containment relation between a place and one that contains it.\" ;\n    schema:domainIncludes schema:Place ;\n    schema:inverseOf schema:containsPlace ;\n    schema:rangeIncludes schema:Place .\nThis metadata is part of an ontology. An ontology is an “explicit specification of a conceptualization” (TKTK ref Gruber) of a domain, including the “the types, properties, and interrelationships of entities that exist for a particular domain of discourse” (TKTK ref Gene Ontology Consortium). An ontology includes machine-readable specifications as well as human-readable labels and definitions that codify shared understanding of a domain. In the snippet above the domain is geography – places and a valid relationship (contained-in) between them. The ontology is not about specific places, but about the category “place”, its possible properties, and how it relates to other categories. The example eliminates ambiguity what schema:containedIn means through the use of a human-readable definition (“The basic containment relation …”) as well as machine-enforceable constraints on the categories of things that can be involved in the schema:containedInPlace relation (the schema:domainIncludes and schema:rangeIncludes). By constraining the domain of discourse an ontology amplifies the benefits of reusing identifiers by providing further clarity to humans as well as a way for machines to ensure that data and metadata stay within those constraints.\n\nAn ontology of economics\nOntologies like the Financial Industry Business Ontology are commonly used in the financial services sector to enable cross-system federation and aggregation of data in order to support decision-making, streamline regulatory reporting, and encourage the adoption of advanced analytical capabilities.\nUnfortunately, the macroeconomics domain does not have an ontology comparable in scope and rigour to FIBO. We recommend developing an ontology of macroeconomics incrementally. At a minimum this ontology should model:\n\nthe structure of statistical data cubes (dimensions, attributes, measures)\ncommon dimensions such as time, place, and macroeconomic indicators\ncommon attributes such as units\n\nIn the long term the ontology should also model:\n\nrelationships between dimensions e.g., different macroeconomic indicators\nrelationships between attributes e.g., unit conversions\ndata lineage and provenance\nmodeling assumptions\ninformation about economic organizations, people, and the relationships between them (a social network)\ninformation about places and their relationships (e.g., aggregates such as Low Income Countries)\nmachine-readable assertions extracted from human-written web pages and other documents (a knowledge graph about the economic world)\n\nFortunately, there are a number of existing standards that can serve as building blocks for the ontology, including:\n\nSDMX Data Structure Definitions (DSDs) e.g., Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)\nthe RDF Data Cube Vocabulary for publishing multi-dimensional statistical data as Linked Data\nthe PROV Ontology of data lineage and provenance\nQUDT ontologies of units of measure, quantity kind, dimensions and data types\nSKOS specifications and standards for knowledge organization systems (KOS) (thesauri, taxonomies, et al.)\nthe VoID vocabulary for describing linked datasets\nSchema.org cross-domain vocabularies"
  },
  {
    "objectID": "report.html#tools-to-streamline-economic-data-reuse",
    "href": "report.html#tools-to-streamline-economic-data-reuse",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Tools to streamline economic data reuse",
    "text": "Tools to streamline economic data reuse\nIncreasing semantic interoperability, developing ontologies, and taking advantage of linked data will reduce the toil and increase the productivity of using existing tools in the EconDataverse. However, in order to fully address the challenges highlighted in TKTK section ref, we also recommend expanding and refining the EconDataverse toolkit in three key areas:\n\ntools for extracting and transforming economic data and metadata\ntools for exploring and finding economic data and metadata\ntools for analyzing and manipulating financial models\n\nNew tools in these areas should produce and consume data and metadata that conform to the ontology in order to enable new features, like providing unified views over heterogeneous data sources.\nFor the short- to mid-term we recommend implementing an adapter library and command-line program that transforms ontology-conformant data to and from the formats expected by existing EconDataverse tools. In the long term these tools should be retrofitted to produce and consume ontology-conformant data and metadata.\n\nTools for extracting and transforming economic data and metadata\nThe majority of packages in the current EconDataverse package ecosystem extract data from IMF, World Bank, and other sources and transform them into tidy data frames, with each variable as a column and each observation as a row. Metadata are typically limited to column names, which are not explicitly grounded in an ontology that would provide context and definitions.\nGoing forward, these packages should produce ontology-conformant data and metadata that fully capture the semantics of the source data. These can be easily but lossily transformed into convenient but semantics-poor formats like tidy data frames for compatibility with third party tools.\nExtraction should not be limited to tabular data sources. The ontology can also be used to guide Large Language Models (LLMs) in the extraction of ontology-conformant structured data from natural language sources such as the World Bank country and climate development reports.\n\n\nTools for exploring and finding economic data and metadata\nTools for exploring and finding data and metadata tend to be useful in proportion to how closely the data’s representation corresponds to the domain abstractions, like accounts and sub-accounts in Balance of Payments datasets. This requires a level of abstraction and interpretation that is difficult to achieve without building data source-specific tools. Instead, most generic tools work with data and metadata at the lowest common denominator of abstraction – as tables and rows of figures to be scanned or a corpus of arbitrary text to be searched. The problem of interpretation is mostly left up to the user.\nHaving data and metadata from multiple sources conform to a rich, domain-specific ontology makes it possible to build tools that work at a higher level of abstraction without sacrificing source independence. Tools can make assumptions about the meaning of the data and relationships between them, which dramatically increases the tools’ power. For example, the next generation of EconDataverse tools could:\n\ngroup or aggregate data from different (geographic) reference areas by considering explicit relationships between areas, such as part-whole or collection membership\nlet users browse and filter cube-structured datasets and slices by the dimensions they incorporate (e.g., time, place, gender), provenance, or recency\nsuggest inputs for a given model by matching metadata about the inputs with metadata from a data/metadata catalog\n\n\n\nTools for analyzing and manipulating financial models\nFinally, the EconDataverse needs tools for analyzing and financial models, which are typically encoded in Excel spreadsheets. In the short term, tools should automate or semi-automate toilsome tasks like:\n\nManually mapping statistical data cube measures to spreadsheet inputs\nManually reverse-engineering spreadsheets to identify which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs\nManually permuting spreadsheets inputs in order to perform sensitivity analyses\n\n\n\nDisambiguating inputs\nMuch of the toil in using financial models lies in the process of identifying the right source data to supply as model inputs. For example, the Debt Dynamics Toolkit spreadsheet requires the following inputs in a single sheet:\n\nDynamic Debt Toolkit inputs sheet\n\n\n\n\n\n\n\nYear / Variable\n2011\n2012\n\n\n\n\ndt (debt including uncalled guarantees): stock of total gross public debt, percent of GDP\n31.59\n35.34\n\n\no/w stock of local-currency guarantees (uncalled): stock of uncalled guarantees in local currency included in total debt, percent of GDP\n0.00\n0.00\n\n\no/w stock of foreign-currency guarantees (uncalled): stock of uncalled guarantees in foreign currency included in total debt, percent of GDP\n0.00\n0.00\n\n\nαt (share excl. guarantees): share of foreign currency denominated debt in total debt, percent of total debt\n55.05\n58.87\n\n\net (LCU/FCU, avg): nominal average exchange rate, local currency per unit of foreign currency\n18.92\n19.50\n\n\net (LCU/FCU, eop): nominal end of period exchange rate, local currency per unit of foreign currency\n19.05\n19.96\n\n\nitd: nominal effective interest rate on local currency denominated debt, percent\n8.92\n9.23\n\n\nitf: nominal effective interest rate on foreign currency denominated debt, percent\n2.46\n2.03\n\n\nπt: GDP deflator inflation, percent\n5.60\n5.39\n\n\ngt: Real GDP growth, percent\n3.84\n4.13\n\n\npbt: Primary balance, percent of GDP\n-3.28\n-4.03\n\n\noft (other net debt-creating flows): Other net debt creating flows, percent of GDP\n0.00\n0.00\n\n\nπft: Foreign GDP deflator inflation, percent (used in fan chart)\n2.09\n1.92\n\n\n\nPer the data cube model, each input cell can be uniquely identified by dimensions:\n\nTime: with controlled values 2011 and 2012\nPlace: implicitly, the country whose debt is being analyzed\nIndicator: such as “Real GDP growth”\n\nIn this case the year and place are unambiguously identified and are relatively easy to match to source data. It is not as obvious how to map the indicator “nominal effective interest rate on foreign currency denominated debt, percent” to a specific source database.\nOne approach is to treat “nominal effective interest rate on foreign currency denominated debt, percent” as a unique identifier as-is, like Google Data Commons does. This assumes that the source data sources are also using this identifier, which is unlikeliy.\nThe approach we recommend is to decompose the name into a structured combination of sub-identifiers:\n\na base indicator: “interest rate”\nqualifiers on the base indicator: “nominal”, “effective”, “on foreign currency denominated debt”\nattributes: “percent”, which should be treated as an attribute of the data rather than as part of the indicator dimension\n\nThe structure and valid combinations of indicator, qualifier, and attribute values should be dictated by the ontology. Given a permissible set of controlled values, a Large Language Model could aid in this decomposition from natural language.\nThis approach parallels the way World Bank debt codes can be decomposed into segments. For example, DT.DOD.[Debt Instrument].[Debtor/Creditor Sector].[Unit] can be decomposed into:\n\nDT: Debt\nDOD: Debt Outstanding and Disbursed\nInstrument Type (e.g., long-term, short-term, use of IMF credit)\nDebtor or Creditor (e.g., public, private, multilateral, bilateral)\nUnit of measure (e.g., CD = current US dollars, GD.ZS = % of GNI)\n\n\n\nModels as data\nIn the long term the financial models themselves should be treated as data, and extracted and transformed into ontology-conformant data and metadata. From there they could be lossily transformed back into spreadsheets. This approach would build on open source precedents like Morphir, a system that captures an application’s domain model and business logic in a technology agnostic manner."
  },
  {
    "objectID": "report.html#automated-code-quality-tools",
    "href": "report.html#automated-code-quality-tools",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Automated code quality tools",
    "text": "Automated code quality tools\nWe recommend immediately incorporating code quality tools into existing EconDataverse development workflows, such as mypy type checking, ruff linting, and black formatting for Python. The other code quality tools and practices described in the previous section should be gradually adopted."
  },
  {
    "objectID": "report.html#documentation",
    "href": "report.html#documentation",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Documentation",
    "text": "Documentation\nMissing and sparse documentation are a clear barrier to adopting EconDataverse tools. For the short term we recommend standardizing the structure and style of existing documentation (primarily READMEs and inline function/method documentation) and adopting the Vale documentation linter."
  },
  {
    "objectID": "report.html#ontology-of-economics",
    "href": "report.html#ontology-of-economics",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Ontology of economics",
    "text": "Ontology of economics\nWe recommend curating a minimum viable ontology of economics that is sufficient to losslessly capture data originating and metadata originating from key data sources:\n\nSDMX\nGoogle Data Commons\nthe World Bank’s World Development Indicators (WDI) API\nthe IMF’s World Economic Outlook (WEO) database"
  },
  {
    "objectID": "report.html#adapter-library-and-command-line-program",
    "href": "report.html#adapter-library-and-command-line-program",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Adapter library and command line program",
    "text": "Adapter library and command line program\nWe recommend developing an adapter library in Python and a command line program that uses the library to transform data and metadata to and from an ontology-conformant representation and other formats:\n\nSDMX-CSV\nGoogle Data Commons\nRDF\nThe tidy tabular formats produced and consumed by existing EconDataverse tools (as a compatibility stop-gap)"
  },
  {
    "objectID": "report.html#tool-for-extracting-ontology-conformant-data-from-documents",
    "href": "report.html#tool-for-extracting-ontology-conformant-data-from-documents",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Tool for extracting ontology-conformant data from documents",
    "text": "Tool for extracting ontology-conformant data from documents\nWe recommend implementing a new tool for extracting ontology-conformant data from a document corpus such as the IMF’s Debt Sustainability Analysis Low-Income Countries (DSA LIC) reports. The reports comprise a mixture of narrative text, rendered tables, and charts as PDFs. The first release of the tool should focus on extracting statistical data cube-compatible data (dimensions, attributes, measures) from tables in the documents. We recommend experimenting with different libraries and APIs such as pdfplumber and AWS Textract to identify a solution that maximizes key extraction metrics (precision, recall, F1 score)."
  },
  {
    "objectID": "report.html#tool-for-exploring-and-finding-ontology-conformant-data-and-metadata",
    "href": "report.html#tool-for-exploring-and-finding-ontology-conformant-data-and-metadata",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Tool for exploring and finding ontology-conformant data and metadata",
    "text": "Tool for exploring and finding ontology-conformant data and metadata\nWe recommend designing and developing a full-stack web application for exploring and finding ontology-conformant data and metadata. The application should consist of:\n\na database aggregated from multiple sources via the aforementioned adapter library and extraction tool\na reusable library mediating database access\na graphical user interface that offers both full-text search and faceted search\n\nSearch functionality should be benchmarked against standard information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG).\nSubsequent application releases could include additional functionality, such as:\n\na conversational user interface\none or more programmatic interfaces (APIs), such as a SPARQL endpoint or an SDMX Registry\nsupport for querying multiple public and private databases, such as Google Data Commons\n\nGiven the relatively small volume of data and their public nature, we recommend considering a static site generation framework such as Next.js in order the costs (financial, labor) associated with hosting the application."
  },
  {
    "objectID": "report.html#tool-for-identifying-minimal-spreadsheet-inputs",
    "href": "report.html#tool-for-identifying-minimal-spreadsheet-inputs",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Tool for identifying minimal spreadsheet inputs",
    "text": "Tool for identifying minimal spreadsheet inputs\nWe recommend identifying or developing a tool that accepts a spreadsheet-based financial model such as the LIC DSF or the Dynamic Debt Toolkit and identifies which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs. There are two fundamental approaches to this problem:\n\nWhite box: build a dependency graph of spreadsheet formulas spanning the inputs and outputs in order to identify which inputs the desired outputs depend on. This approach is likely to produce false positives – flagging a set of inputs larger than the true minimum – because it is insensitive to the magnitude of input changes.\nBlack box: mutate individual inputs and combinations of inputs and record which outputs change. This approach may produce false negatives – flagging a set of inputs smaller than the true minimum – because of untested interactions between inputs.\n\nIdeally a tool will utilize both approaches to check each other."
  },
  {
    "objectID": "sections/executive-summary.html",
    "href": "sections/executive-summary.html",
    "title": "Executive summary",
    "section": "",
    "text": "Executive summary"
  },
  {
    "objectID": "sections/problem.html",
    "href": "sections/problem.html",
    "title": "Problem",
    "section": "",
    "text": "Problem\nSovereign debt analysis projects rely heavily on the analysis, processing, and storage of macroeconomic datasets. However, the current data-driven technologies and processes used by Debt Management Offices (DMOs) and sovereign analysts are often outdated, fragmented, and inefficient. These limitations are especially burdensome for resource-constrained institutions that are working to efficiently and effectively enhance debt analysis capacity.\nA significant portion of the work required for sovereign analysis qualifies as toil: manual, repetitive, low-leverage tasks that are often automatable and devoid of long-term value. Toil scales linearly with project complexity, meaning that as the scope of sovereign advisory efforts expands, so too does the operational burden. This introduces significant wasted time and effort into a sovereign analyst’s work and weakens their ability to accomplish high-impact analytical work.\n\nData Processes and Technologies\nSovereign analysts depend on macroeconomic datasets from:\n\nThe International Monetary Fund\nThe World Bank\nOther national and international organizations\n\nHowever, this data is often siloed, difficult to find, hard to access, and/or poorly optimized for reuse. In many cases, the data needed to answer a pressing policy question is technically available but practically inaccessible—buried within PDFs, complex spreadsheets, or outdated web portals.\n\nThe FAIR Guiding Principles for scientific data management and stewardship (TKTK ref) provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets, with the ultimate goal of optimizing the reuse of data. Macroeconomic data management confronts many of the same challenges as scientific data management, such as: * finding and accessing the right data * integrating heterogeneous data sources on a semantic level * dealing with missing and low-quality metadata * tracking data provenance\n\nFindability &gt; Once the user finds the required data, she/he/they need to know how they can be accessed, possibly including authentication and authorisation.\nAccessibility &gt; Once the user finds the required data, she/he/they need to know how they can be accessed, possibly including authentication and authorisation.\nInteroperability &gt; The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\nReusability\nMetadata and data should be well-described so that they can be replicated and/or combined in different settings.\n\nThis fragmented data environment complicates fundamental tasks such as harmonizing multiple datasets, preserving data quality, or combining structured and unstructured information. Sovereign analysts may spend hours, if not days, locating, extracting, and validating data before any meaningful analysis can begin. The result is not only wasted time and effort, but also delayed insights that could inform critical policy choices.\n\n\nSpreadsheet-based Models\nFurther compounding the issue is the underdeveloped analytical tools for macroeconomic modeling. Many key workflows still rely on spreadsheet-based financial models, such as the Low-Income Country Debt Sustainability Framework (LIC-DSF) and the Debt Dynamics Tool (DDT).\nThese models are often opaque and complex, with multivariate inputs linked to policy-relevant outputs through undocumented, hard-to-follow logic.\nThere is typically no clear record of how input variables influence key indicators, or how one might efficiently trace through the model to answer a specific policy question.\nThis forces analysts to spend valuable time and effort on reverse-engineering spreadsheets.\nFor example, in the LIC-DSF, the process of identifying the appropriate inputs for variables such as “Indicators of Public and Publicly Guaranteed External Debt under Alternative Scenarios” and “Indicators of Public Debt Under Alternative Scenarios”—both essential for conducting macroeconomic stress tests and assessing a country’s debt-carrying capacity—typically involves:\n\nmanually narrowing down relevant inputs to infer relationships\nmanually validating those inferences\nrecreating the validated logic across new spreadsheets to replicate results\n\nThis process is highly labor-intensive, error-prone, and non-replicable at scale. The absence of automation or documentation not only creates inefficiencies, but also introduces risks to the consistency and credibility of the analysis.\nModern AI technologies and advanced data systems offer an opportunity to reimagine the way sovereign analysis is done. These technologies have the capabilities to address these challenges to enhance the efficiency, accuracy, and impact of sovereign advisory work. By automating repetitive tasks and streamlining data workflows, these tools can eliminate toil and free up a sovereign analyst’s time for more high-value work."
  },
  {
    "objectID": "sections/solution-criteria.html",
    "href": "sections/solution-criteria.html",
    "title": "Solution criteria",
    "section": "",
    "text": "A successful solution should save time and effort on data-driven sovereign advisor projects by eliminating unnecessary toil from sovereign advisory projects. In particular, we will focus on opportunities to streamline the use and reuse of data, processes which currently induce considerable toil.\n\n\n\nwith other EconDataverse tools in Python and R\nTidy Data Outputs All packages return data in a tidy format, with each variable as a column, each observation as a row, and each data type as a table. Defaulting to tidy data ensures compatibility with popular data manipulation and visualization libraries in both R and Python.\n\nWherever feasible, EconDataverse interfaces are designed to work across languages. Consistency across languages enables analysts and researchers to incorporate these tools into various projects, regardless of language preference, and fosters a collaborative, multi-lingual community.\nEvery package within the EconDataverse follows a standardized interface with predictable patterns. For instance, functions for data retrieval are named consistently: *_get functions retrieve data from a specific source, *_list functions enumerate available resources.\nFor R packages, the EconDataverse adopts the tidyverse coding style and enforces it through {lintr}. For Python, we use the same naming conventions, but respect style conventions layed out in PEP 8. These styles enhance the readability of code and align with the practices of users familiar with the corresponding language.\n\n\n\n\n\n\nUnix philosophy: do one thing well No kitchen sinks\n\n\n\nIterative development\nEach package is thoroughly tested through extensive unit tests, which ensure consistent functionality, protect against regressions, and help users trust that the packages will work as expected across different environments.\n\n\n\nAccommodate future data sources, …\n\n\n\nIn terms of people / development as well as data volume\n\n\n\n\n\n\nAll packages are released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community."
  },
  {
    "objectID": "sections/solution-criteria.html#streamlining-data-reuse",
    "href": "sections/solution-criteria.html#streamlining-data-reuse",
    "title": "Solution criteria",
    "section": "",
    "text": "with other EconDataverse tools in Python and R\nTidy Data Outputs All packages return data in a tidy format, with each variable as a column, each observation as a row, and each data type as a table. Defaulting to tidy data ensures compatibility with popular data manipulation and visualization libraries in both R and Python.\n\nWherever feasible, EconDataverse interfaces are designed to work across languages. Consistency across languages enables analysts and researchers to incorporate these tools into various projects, regardless of language preference, and fosters a collaborative, multi-lingual community.\nEvery package within the EconDataverse follows a standardized interface with predictable patterns. For instance, functions for data retrieval are named consistently: *_get functions retrieve data from a specific source, *_list functions enumerate available resources.\nFor R packages, the EconDataverse adopts the tidyverse coding style and enforces it through {lintr}. For Python, we use the same naming conventions, but respect style conventions layed out in PEP 8. These styles enhance the readability of code and align with the practices of users familiar with the corresponding language."
  },
  {
    "objectID": "sections/solution-criteria.html#modularity",
    "href": "sections/solution-criteria.html#modularity",
    "title": "Solution criteria",
    "section": "",
    "text": "Unix philosophy: do one thing well No kitchen sinks"
  },
  {
    "objectID": "sections/solution-criteria.html#stability",
    "href": "sections/solution-criteria.html#stability",
    "title": "Solution criteria",
    "section": "",
    "text": "Iterative development\nEach package is thoroughly tested through extensive unit tests, which ensure consistent functionality, protect against regressions, and help users trust that the packages will work as expected across different environments."
  },
  {
    "objectID": "sections/solution-criteria.html#flexibility",
    "href": "sections/solution-criteria.html#flexibility",
    "title": "Solution criteria",
    "section": "",
    "text": "Accommodate future data sources, …"
  },
  {
    "objectID": "sections/solution-criteria.html#scalability",
    "href": "sections/solution-criteria.html#scalability",
    "title": "Solution criteria",
    "section": "",
    "text": "In terms of people / development as well as data volume"
  },
  {
    "objectID": "sections/solution-criteria.html#transparency",
    "href": "sections/solution-criteria.html#transparency",
    "title": "Solution criteria",
    "section": "",
    "text": "All packages are released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community."
  }
]