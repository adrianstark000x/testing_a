[
  {
    "objectID": "sections/solution.html",
    "href": "sections/solution.html",
    "title": "Solution",
    "section": "",
    "text": "In the following sections we recommend a solution that entails: * increasing semantic interoperability between tools in the EconDataverse * linking data to other data and metadata * constraining data and metadata using ontologies * developing new tools that rely on semantic interoperability, linked data, and ontologies to streamline economic data reuse"
  },
  {
    "objectID": "sections/solution.html#semantic-interoperability",
    "href": "sections/solution.html#semantic-interoperability",
    "title": "Solution",
    "section": "Semantic interoperability",
    "text": "Semantic interoperability\nThe EconDataverse is both a set of software tools in R and Python and a set of conventions and best practices for developing tools that work with economic data. There are coding conventions that ensure consistent style across packages as well as data syntax conventions that ensure compatibility with popular data manipulation and visualization libraries in both R and Python.\nInteroperability between tools in the EconDataverse is a key concern, since deficiencies in interoperability create additional toil for the user. In the EconDataverse the problem of interoperability is one of ensuring the tools “speak the same language”. A simple approach to this problem is to consider language as equivalent to syntax, and ensure that tools consume and produce the same syntax, such as the tidy format (TKTK ref) espoused by current EconDataverse tools. This reduces the toil associated with transforming syntax between tools but leaves the problem of semantic interoperability.\nIn this context semantic interoperability is the ability of different tools to exchange data with unambiguous, shared meaning. It ensures that the meaning of the data is preserved and correctly interpreted between tools, even if those tools use different technologies, such as R and Python.\nSemantic interoperability starts from agreeing on unambiguous identifiers for things. For example, the EconDataverse package maintainers have agreed to refer to countries using ISO 3166-1 standard codes and to represent years using four digits and the Gregorian calendar. This consensus ensures that data from different sources about the same countries and years can be joined, as in the R example below:\n\n\n\n\n\n\nNote\n\n\n\nTeal: ChatGPT generated this example. It’d be great to have a better one along the same lines.\n\n\n# Load required package\nlibrary(dplyr)\n\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# --- Perform inner join on ISO country code and year ---\njoined_data &lt;- inner_join(macro_data, population_data, by = c(\"iso3c\", \"year\"))\nImagine that the structure of the two datasets stayed the same but they used different identifiers for countries: one dataset referred to the United States as “États-Unis” while the other referred to it as “USA”.\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"États-Unis\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# Join on USA data is not going to work as intended here.\nThe data analyst would have to first resolve the discrepancy – mapping “États-Unis” to “USA” or vice versa – in order to harmonize data from multiple sources. That work is toil that can be eliminated by agreeing on unambiguous identifiers and using them consistently.\n\nUniquely identifying numeric data\nThe R example in figure X also includes numeric data such as GDP (in billions of of US Dollars) and population (in millions). Unlike the ISO 3166 country codes, these data have ambiguous meanings. A human data analyst can infer at least part of the meaning from the column name (gdp_billion) and the accompanying comment (“GDP (in USD billions)”). Other parts are still ambiguous – is it nominal or real GDP?\nWe would like to eliminate this ambiguity and the toil required to resolve it by associating numeric data with unambiguous metadata. The SDMX Information Model recommends that the metadata for a numeric datum include:\n\nDimensions whose combined values uniquely identify that datum in a cube/hypercube of dimensions\nAttributes of the datum, such as units of measurement\n\nThe dimensions uniquely identifying a single nominal GDP measurement are:\n\nPlace: the country associated with the GDP (e.g., “USA” and “FRA”)\nTime: the year the GDP was measured (e.g., 2024).\nIndicator: nominal GDP\n\nDimensions like “place” and attributes like “unit of measurement” should themselves be uniquely identified, and have associated properties such as human-readable labels and definitions, relationships with other dimensions or attributes, and constraints on the values that can be associated with a dimension (e.g., code lists). The SDMX Content-Oriented Guidelines provide a library of well-known, uniquely-identified dimensions and attributes that can be reused in many different domains."
  },
  {
    "objectID": "sections/solution.html#linking-data",
    "href": "sections/solution.html#linking-data",
    "title": "Solution",
    "section": "Linking data",
    "text": "Linking data\nWe can use the unambiguous identifier for a thing (“USA”) in the data as a key to resolve additional properties about the thing as well as its relationships with other things, following the Linked Data principles. For example, we can use schema.org vocabularies to describe the relationship between the country of the United States and the continent of North America, in a machine-readable format (an RDF knowledge graph represented in Turtle):\n@prefix ex: &lt;http://example.org/entities/&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\n# United States of America\nex:USA a schema:Country ;\n    # Human-readable labels in multiple human languages\n    schema:name \"United States of America\"@en ;\n    schema:name \"États-Unis\"@fr ;\n    schema:identifier \"USA\" ;\n    # A relationship: the country United States is contained in the place North America.\n    schema:containedInPlace ex:NA .\n\n# North America\nex:NA a schema:Continent ;\n    schema:name \"North America\"@en ;\n    schema:identifier \"NA\" ."
  },
  {
    "objectID": "sections/solution.html#constraining-data-and-metadata",
    "href": "sections/solution.html#constraining-data-and-metadata",
    "title": "Solution",
    "section": "Constraining data and metadata",
    "text": "Constraining data and metadata\nThere is an additional level of consensus at work in the example above. By referencing schema.org we are implicitly agreeing that the unambiguous identifier schema:containedInPlace refers to a way of relating two places. That identifier can be used in turn as a key to further linked metadata:\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\nschema:containedInPlace a rdf:Property ;\n    rdfs:label \"containedInPlace\" ;\n    rdfs:comment \"The basic containment relation between a place and one that contains it.\" ;\n    schema:domainIncludes schema:Place ;\n    schema:inverseOf schema:containsPlace ;\n    schema:rangeIncludes schema:Place .\nThis metadata is part of an ontology. An ontology is an “explicit specification of a conceptualization” (TKTK ref Gruber 1995) of a domain, including the “the types, properties, and interrelationships of entities that exist for a particular domain of discourse” (TKTK ref Gene Ontology Consortium). An ontology includes machine-readable specifications as well as human-readable labels and definitions that codify shared understanding of a domain. In the snippet above the domain is geography – places and a valid relationship (contained-in) between them. The ontology is not about specific places, but about the category “place”, its possible properties, and how it relates to other categories. The example eliminates ambiguity what schema:containedInPlace means through the use of a human-readable definition (“The basic containment relation …”) as well as machine-enforceable constraints on the categories of things that can be involved in the schema:containedInPlace relation (the schema:domainIncludes and schema:rangeIncludes). By constraining the domain of discourse an ontology amplifies the benefits of reusing identifiers by providing further clarity to humans as well as a way for machines to ensure that data and metadata stay within those constraints.\n\nAn ontology of economics\nOntologies like the Financial Industry Business Ontology (FIBO) are commonly used in the financial services sector to enable cross-system federation and aggregation of data in order to support decision-making, streamline regulatory reporting, and encourage the adoption of advanced analytical capabilities.\nUnfortunately, the macroeconomics domain does not have an ontology comparable in scope and rigour to FIBO. We recommend developing an ontology of macroeconomics incrementally. At a minimum this ontology should model:\n\nthe structure of statistical data cubes (dimensions, attributes, measures)\ncommon dimensions such as time, place, and macroeconomic indicators\ncommon attributes such as units\n\nIn the long term the ontology should also model:\n\nrelationships between dimensions e.g., different macroeconomic indicators\nrelationships between attributes e.g., unit conversions\ndata lineage and provenance\nmodeling assumptions\ninformation about economic organizations, people, and the relationships between them (a social network)\ninformation about places and relationships between them (e.g., aggregates such as Low Income Countries)\nmachine-readable assertions extracted from human-written web pages and other documents (a knowledge graph about the economic world)\n\nFortunately, there are a number of existing standards that can serve as building blocks for the ontology, including:\n\nSDMX Data Structure Definitions (DSDs) e.g., Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)\nthe RDF Data Cube Vocabulary for publishing multi-dimensional statistical data as Linked Data\nthe PROV Ontology of data lineage and provenance\nQUDT ontologies of units of measure, quantity kind, dimensions and data types\nSKOS specifications and standards for knowledge organization systems (KOS) (thesauri, taxonomies, et al.)\nthe VoID vocabulary for describing linked datasets\nSchema.org cross-domain vocabularies"
  },
  {
    "objectID": "sections/solution.html#tools-to-streamline-economic-data-reuse",
    "href": "sections/solution.html#tools-to-streamline-economic-data-reuse",
    "title": "Solution",
    "section": "Tools to streamline economic data reuse",
    "text": "Tools to streamline economic data reuse\nIncreasing semantic interoperability, developing ontologies, and taking advantage of linked data will reduce the toil and increase the productivity of using existing tools in the EconDataverse. However, in order to fully address the challenges highlighted in ?@sec-solution-criteria, we also recommend expanding and refining the EconDataverse toolkit in three key areas:\n\ntools for extracting and transforming economic data and metadata\ntools for exploring and finding economic data and metadata\ntools for analyzing and manipulating financial models\n\nFigure Figure 1 illustrates our proposal for a new high-level system architecture for EconDataverse tools.\n\n\nWarning: node 'cf02cf40d1234baaae2737aecbf98d7e', graph '%3' size too small for label\nWarning: node '54244f63393744bc8a7f72f55de4589b', graph '%3' size too small for label\n\n\n\n\n\n\n\n\nFigure 1: Recommended EconDataverse system architecture\n\n\n\n\n\nGreen lines indicate the flow of ontology-conformant data and metadata while red lines indicate the flow of data in other formats.\nThe following subsections delve into the proposed tool developments in more detail.\n\nTools for extracting, transforming, and loading economic data and metadata\nThe majority of packages in the current EconDataverse package ecosystem extract structured data from IMF, World Bank, and other sources and transform them into tidy format, with each variable as a column and each observation as a row. Metadata are typically limited to column names, which are not explicitly grounded in an ontology that would provide context and definitions. With the notable exception of ISO 3166 country codes, there is relatively little standardization of non-numeric data across data sources.\nIn the short- to mid-term we recommend implementing an adapter library and command-line program that transforms ontology-conformant data to and from the formats expected by existing EconDataverse tools. In the long term these tools should be retrofitted to produce and consume ontology-conformant data and metadata.\nNew tools in the EconDataverse should produce ontology-conformant data and metadata that fully capture the semantics of the source data. These can be easily but lossily transformed into convenient but semantics-poor formats like tidy data frames for compatibility with third party tools.\nExtraction should not be limited to structured data sources. The ontology can also be used to guide Large Language Models (LLMs) in the extraction of ontology-conformant structured data from natural language sources such as the World Bank country and climate development reports or the IMF’s Debt Sustainability Analysis Low-Income Countries (DSA LIC reports). The latter comprise a mixture of narrative text, rendered tables, and charts as PDFs. We recommend starting by extracting statistical data cube-compatible data (dimensions, attributes, measures) from tables in the documents and experimenting with different libraries and APIs such as pdfplumber and AWS Textract to identify a toolchain that maximizes key extraction metrics such as precision, recall, and F1 score.\nOntology-conformant data and metadata extracted and transformed by new and existing tools should be loaded into a store that can preserve the full richness of the data, such as an RDF triple store. The tools described in the next subsection should query this store.\n\n\nTools for exploring and finding economic data and metadata\nTools for exploring and finding data and metadata tend to be useful in proportion to how closely the data’s representation corresponds to the domain abstractions, like accounts and sub-accounts in Balance of Payments datasets. This requires a level of abstraction and interpretation that is difficult to achieve without building data source-specific tools. Instead, most generic tools work with data and metadata at the lowest common denominator of abstraction – as tables and rows of figures to be scanned or a corpus of arbitrary text to be searched. The problem of interpretation is mostly left up to the user.\nHaving data and metadata from multiple sources conform to a rich, domain-specific ontology makes it possible to build tools that work at a higher level of abstraction without sacrificing source independence. Tools can make assumptions about the meaning of the data and relationships between them, which dramatically increases the tools’ power. For example, the next generation of EconDataverse tools could:\n\ngroup or aggregate data from different (geographic) reference areas by considering explicit relationships between areas, such as part-whole or collection membership\nlet users browse and filter cube-structured datasets and slices by the dimensions they incorporate (e.g., time, place, gender), provenance, or recency\nsuggest inputs for a given model by matching metadata about the inputs with metadata from a data/metadata catalog\nsupport federated querying of multiple public and private databases, such as Google Data Commons\n\nThese user-facing tools could have different interfaces or combinations of interfaces: * command line programs * graphical user interfaces * a conversational user interface * programmatic interfaces (APIs), such as a SPARQL endpoint or an SDMX Registry\n\n\nTools for analyzing and manipulating financial models\nFinally, the EconDataverse needs tools for analyzing and financial models, which are typically encoded in Excel spreadsheets. In the short term, tools should automate or semi-automate toilsome tasks like:\n\nManually reverse-engineering spreadsheets to identify which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs\nManually mapping statistical data cube measures to spreadsheet inputs\nManually changing spreadsheets inputs in order to perform sensitivity analyses\n\n\n\nIdentifying minimal spreadsheet inputs\nWe recommend identifying or developing a tool that accepts a spreadsheet-based financial model such as the LIC DSF or the Dynamic Debt Toolkit and identifies which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs. There are two fundamental approaches to this problem:\n\nWhite box: build a dependency graph of spreadsheet formulae spanning the inputs and outputs in order to identify which inputs the desired outputs depend on. This approach is likely to produce false positives – flagging a set of inputs larger than the true minimum – because it is insensitive to the magnitude of input changes.\nBlack box: mutate individual inputs and combinations of inputs and record which outputs change. This approach may produce false negatives – flagging a set of inputs smaller than the true minimum – because of untested interactions between inputs.\n\nIdeally a tool will utilize both approaches to check each other.\n\n\nDisambiguating inputs\nMuch of the toil in using financial models lies in the process of identifying the right source data to supply as model inputs. For example, the Debt Dynamics Toolkit spreadsheet requires the following inputs in a single sheet:\n\nDynamic Debt Toolkit inputs sheet\n\n\n\n\n\n\n\nYear / Variable\n2011\n2012\n\n\n\n\ndt (debt including uncalled guarantees): stock of total gross public debt, percent of GDP\n31.59\n35.34\n\n\no/w stock of local-currency guarantees (uncalled): stock of uncalled guarantees in local currency included in total debt, percent of GDP\n0.00\n0.00\n\n\no/w stock of foreign-currency guarantees (uncalled): stock of uncalled guarantees in foreign currency included in total debt, percent of GDP\n0.00\n0.00\n\n\nαt (share excl. guarantees): share of foreign currency denominated debt in total debt, percent of total debt\n55.05\n58.87\n\n\net (LCU/FCU, avg): nominal average exchange rate, local currency per unit of foreign currency\n18.92\n19.50\n\n\net (LCU/FCU, eop): nominal end of period exchange rate, local currency per unit of foreign currency\n19.05\n19.96\n\n\nitd: nominal effective interest rate on local currency denominated debt, percent\n8.92\n9.23\n\n\nitf: nominal effective interest rate on foreign currency denominated debt, percent\n2.46\n2.03\n\n\nπt: GDP deflator inflation, percent\n5.60\n5.39\n\n\ngt: Real GDP growth, percent\n3.84\n4.13\n\n\npbt: Primary balance, percent of GDP\n-3.28\n-4.03\n\n\noft (other net debt-creating flows): Other net debt creating flows, percent of GDP\n0.00\n0.00\n\n\nπft: Foreign GDP deflator inflation, percent (used in fan chart)\n2.09\n1.92\n\n\n\nPer the data cube model, each input cell can be uniquely identified by dimensions:\n\nTime: with controlled values 2011 and 2012\nPlace: implicitly, the country whose debt is being analyzed\nIndicator: such as “Real GDP growth”\n\nIn this case the year and place are unambiguously identified and are relatively easy to match to source data. It is not as obvious how to map the indicator “nominal effective interest rate on foreign currency denominated debt, percent” to source data.\nOne approach is to treat “nominal effective interest rate on foreign currency denominated debt, percent” as a unique identifier as-is, like Google Data Commons does. This is likely to work for some simple indicators like “nominal GDP” but falters on more complex indicators that could be described in different ways, like this example.\nThe approach we recommend is to decompose the name into a structured combination of sub-identifiers:\n\na base indicator: “interest rate”\nqualifiers on the base indicator: “nominal”, “effective”, “on foreign currency denominated debt”\nattributes: “percent”, which should be treated as an attribute of the data rather than as part of the indicator dimension\n\nThe structure and valid combinations of indicator, qualifier, and attribute values should be dictated by the ontology. Given a permissible set of controlled values, a Large Language Model could aid in this decomposition from natural language.\nThis approach parallels the way World Bank debt codes can be decomposed into segments. For example, the World Bank debt code DT.DOD.[Debt Instrument].[Debtor/Creditor Sector].[Unit] can be decomposed into:\n\nDT: Debt\nDOD: Debt Outstanding and Disbursed\nInstrument Type (e.g., long-term, short-term, use of IMF credit)\nDebtor or Creditor (e.g., public, private, multilateral, bilateral)\nUnit of measure (e.g., CD = current US dollars, GD.ZS = % of GNI)\n\n\n\nAutomating sensitivity analyses with spreadsheets\nThe “black box” approach to identifying minimal spreadsheet inputs, described in Section 4.4, can also be used to perform sensitivity analyses by mutating combinations of spreadsheet inputs and recording which outputs change.\n\n\nModels as data, models as code\nIn the long term the financial models themselves should be treated as data, and extracted and transformed into ontology-conformant data and metadata. This approach would build on open source precedents like Morphir, a system that captures an application’s domain model and business logic in a portable / technology-agnostic manner. Given a portable representation of a model and an input dataset, we can execute the model by either:\n\nInterpreting the model representation at runtime.\nGenerating code from the representation and executing the code.\n\nMorphir takes the former approach. We would recommend the latter: lossily generating spreadsheets, Python, R, or other code from ontology types (e.g., dimensions and measures) and a portable model representation. The advantage of this approach is that it would produce an intermediate artifact (generated code) that can be inspected by human users and utilized from non-generated code in a way that ensures model inputs conform to the expected ontology types, above and beyond simple syntax checking."
  },
  {
    "objectID": "sections/recommendations.html",
    "href": "sections/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend implementing the proposed EconDataverse system architecture (?@fig-solution-diagram) iteratively, following agile principles. Rather than focusing on any single part of the system architecture, we recommend developing a set of minimum viable implementations of the different components that together address a useful, end-to-end scenario involving a simple economic model, the IMF’s Dynamic Debt Toolkit (DDT), applied to a reference country (Ghana). Specifically, the tools should help the human analyst:\n\nIdentify a minimum set of spreadsheet inputs for a given output of the DDT template, as described in ?@sec-solution-identifying-minimal-spreadsheet-inputs\nMap the input labels to known data cube dimensions and attributes, as described in ?@sec-solution-disambiguating-inputs\nFind data associated with these dimensions in a store of ontology-conformant data and metadata, as described in ?@sec-solution-ui\n\nA batch process should eagerly populate the store queried by step #3 by extracting, transforming, and loading a subset of relevant ontology-conformant data and metadata (per ?@sec-solution-etl) from at least two data sources, such as\n\nthe World Bank’s World Development Indicators (WDI) API\nthe IMF’s World Economic Outlook (WEO) database\n\nThe aggregation, linking, and subsequent querying of data and metadata from multiple sources will demonstrate the value of ontology-based semantic interoperability between tools.\nThe following subsections recommend specific implementation projects to address the end-to-end scenario.\n\n\nWe recommend identifying or developing a tool that accepts a spreadsheet-based financial model like the Dynamic Debt Toolkit and identifies which inputs are required for a specific subset of desired outputs, in order to avoid spending time populating redundant inputs.\n\n\n\nWe recommend curating a minimum ontology of economics that is sufficient to losslessly capture data and metadata from the selected data sources. The ontology should be specified in SHACL. It should build on the RDF Data Cube Vocabulary as well as existing SDMX Data Structure Definitions such as the Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI).\nFurther, we recommend implementing Python and R libraries for manipulating ontology-conformant data and metadata as type-safe data structures (e.g., classes in Python) and serializing and deserializing data to and from lossless (RDF) and lossy (CSV) formats.\n\n\n\nWe recommend developing pipelines that extract and transform data and metadata from the two sources and load them into an RDF triple store that enforces ontology conformance using SHACL validation. For batch ETL, we recommend utilizing a workflow orchestration tool such as Apache Airflow or Dagster. The pipelines should use the Python and/or R libraries described in the previous subsection. Each phase of a pipeline (extract, transform, load) should be decoupled from the others in modules that can be reused in other contexts (e.g., doing ad hoc extraction and transformation in a computational notebook). Depending on the data sources, these pipelines could be implemented from scratch to produce ontology-conformant data or they could wrap one of the existing EconDataverse libraries, transforming the existing tidy output to ontology-conformant data. .\n\n\n\nWe recommend implementing a full-stack web application for querying the RDF triple store populated by the ETL pipelines. The application should initially consist of a reusable library mediating database access and a graphical user interface that offers multiple forms of search functionality:\n\nfull-text search\nfaceted search\nsemantic search that accepts the string descriptions of inputs in the DDT spreadsheet (c.f., ?@sec-solution-disambiguating-inputs) and maps them to well-known data cube dimensions and attributes\n\nThe search functionality of the application should be benchmarked against standard information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG).\nAfter identifying the desired data / metadata, the user should be able to export them from the web application in one of two forms:\n\nlossless RDF\nlossy tidy CSV"
  },
  {
    "objectID": "sections/recommendations.html#a-tool-for-identifying-minimal-spreadsheet-inputs",
    "href": "sections/recommendations.html#a-tool-for-identifying-minimal-spreadsheet-inputs",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend identifying or developing a tool that accepts a spreadsheet-based financial model like the Dynamic Debt Toolkit and identifies which inputs are required for a specific subset of desired outputs, in order to avoid spending time populating redundant inputs."
  },
  {
    "objectID": "sections/recommendations.html#a-minimum-ontology-of-economics",
    "href": "sections/recommendations.html#a-minimum-ontology-of-economics",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend curating a minimum ontology of economics that is sufficient to losslessly capture data and metadata from the selected data sources. The ontology should be specified in SHACL. It should build on the RDF Data Cube Vocabulary as well as existing SDMX Data Structure Definitions such as the Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI).\nFurther, we recommend implementing Python and R libraries for manipulating ontology-conformant data and metadata as type-safe data structures (e.g., classes in Python) and serializing and deserializing data to and from lossless (RDF) and lossy (CSV) formats."
  },
  {
    "objectID": "sections/recommendations.html#extract-transform-load-etl-pipelines",
    "href": "sections/recommendations.html#extract-transform-load-etl-pipelines",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend developing pipelines that extract and transform data and metadata from the two sources and load them into an RDF triple store that enforces ontology conformance using SHACL validation. For batch ETL, we recommend utilizing a workflow orchestration tool such as Apache Airflow or Dagster. The pipelines should use the Python and/or R libraries described in the previous subsection. Each phase of a pipeline (extract, transform, load) should be decoupled from the others in modules that can be reused in other contexts (e.g., doing ad hoc extraction and transformation in a computational notebook). Depending on the data sources, these pipelines could be implemented from scratch to produce ontology-conformant data or they could wrap one of the existing EconDataverse libraries, transforming the existing tidy output to ontology-conformant data. ."
  },
  {
    "objectID": "sections/recommendations.html#sec-recommendations-web-application",
    "href": "sections/recommendations.html#sec-recommendations-web-application",
    "title": "Recommendations",
    "section": "",
    "text": "We recommend implementing a full-stack web application for querying the RDF triple store populated by the ETL pipelines. The application should initially consist of a reusable library mediating database access and a graphical user interface that offers multiple forms of search functionality:\n\nfull-text search\nfaceted search\nsemantic search that accepts the string descriptions of inputs in the DDT spreadsheet (c.f., ?@sec-solution-disambiguating-inputs) and maps them to well-known data cube dimensions and attributes\n\nThe search functionality of the application should be benchmarked against standard information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG).\nAfter identifying the desired data / metadata, the user should be able to export them from the web application in one of two forms:\n\nlossless RDF\nlossy tidy CSV"
  },
  {
    "objectID": "sections/introduction.html",
    "href": "sections/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nSovereign advisory is the specialized guidance provided to sovereign entities—particularly Ministries of Finance (MOF) and their Debt Management Offices (DMOs)—to strengthen the management of public debt, fiscal policy, and macroeconomic risks. The mission of a sovereign advisor is vital to improving debt sustainability, enhancing access to capital markets, and enabling governments to make informed, data-driven policy decisions that foster long-term economic stability and growth.\nModern technologies offer a powerful pathway forward for transforming sovereign advisory work. Artificial Intelligence (AI) technologies—especially large language models (LLMs)—alongside advanced data systems such as knowledge graphs and ontologies, present a significant opportunity to enhance the capabilities of sovereign analysts. The ability of LLMs to identify and extract information from both structured and unstructured data makes them valuable tools for uncovering insights from complex macroeconomic datasets and policy documents. Knowledge graphs and ontologies structure economic, legal, and financial data in ways that make it more interoperable, findable, and reusable. Together, these technologies can streamline labor-intensive workflows and support more consistent, timely, and data-driven decision-making.\nThis report explores how AI-powered platforms, when combined with robust data technologies, can unlock new levels of efficiency and effectiveness in sovereign advisory. It provides a detailed analysis of current technologies, workflows, and pain points within the sovereign debt management landscape, identifying key bottlenecks that hinder productivity and insight generation. Building on that analysis, it presents a roadmap for developing tools—along with the associated standards, conventions, and best practices—that will help eliminate toil and enhance the efficiency of both sovereign advisors and resource-constrained DMOs.\nFinally, the report offers strategic recommendations for the EconDataverse toolkit. These include long-term foundational technologies and high-level data and software architectures to guide its growth—supporting increased data volume, source heterogeneity, and stakeholder diversity, while maximizing its impact on data-driven sovereign advisory work."
  },
  {
    "objectID": "sections/conclusion.html",
    "href": "sections/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Conclusion"
  },
  {
    "objectID": "figures/solution-diagram.html",
    "href": "figures/solution-diagram.html",
    "title": "",
    "section": "",
    "text": "Warning: node '044e28c965cf485cb1ea29eb37fa897c', graph '%3' size too small for label\nWarning: node '466905e102644d56a946d647a933cf9f', graph '%3' size too small for label\n\n\n\n\n\n\n\n\nFigure 1: Recommended EconDataverse system architecture"
  },
  {
    "objectID": "report.html#modularity",
    "href": "report.html#modularity",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Modularity",
    "text": "Modularity\nA solution should be modular rather than monolithic, with loosely coupled modules that communicate via a well-specified interchange. Each module should “do one thing well”, following the Unix philosophy."
  },
  {
    "objectID": "report.html#frequent-releases",
    "href": "report.html#frequent-releases",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Frequent releases",
    "text": "Frequent releases\nA solution should be iteratively developed, with frequent releases. Module releases should start from a Minimum Viable Product (MVP) and incrementally add functionality on every release, rather than trying to perfect one feature before moving on to another – the “skateboard to car” approach espoused by Henrik Kniberg.\nThorough testing will be essential to ensuring consistent functionality, protecting against regressions, and helping users trust that releases will work as expected across different environments. Tests should cover the entire test pyramid (also c.f., https://testing.googleblog.com/2010/12/test-sizes.html), with unit tests of individual modules as well as integration tests combining modules."
  },
  {
    "objectID": "report.html#flexibility",
    "href": "report.html#flexibility",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Flexibility",
    "text": "Flexibility\nA solution should be flexible enough to accommodate unanticipated future sources of data and financial models. It should avoid premature generalization (YAGNI) as well as over-specialization."
  },
  {
    "objectID": "report.html#scalability",
    "href": "report.html#scalability",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Scalability",
    "text": "Scalability\nA solution should scale in multiple dimensions, including the number of modules, the number of developers, and the volume of data."
  },
  {
    "objectID": "report.html#vendor-neutrality",
    "href": "report.html#vendor-neutrality",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Vendor-neutrality",
    "text": "Vendor-neutrality\nA solution should be as vendor-neutral as possible. It should build on community-maintained open source projects in preference to commercial packages."
  },
  {
    "objectID": "report.html#reuse",
    "href": "report.html#reuse",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Reuse",
    "text": "Reuse\nA solution should avoid needless reinvention and Not Invented Here and take advantage of existing intellectual capital, particularly existing data models."
  },
  {
    "objectID": "report.html#transparency",
    "href": "report.html#transparency",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Transparency",
    "text": "Transparency\nNew code should be released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community."
  },
  {
    "objectID": "report.html#semantic-interoperability",
    "href": "report.html#semantic-interoperability",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Semantic interoperability",
    "text": "Semantic interoperability\nThe EconDataverse is both a set of software tools in R and Python and a set of conventions and best practices for developing tools that work with economic data. There are coding conventions that ensure consistent style across packages as well as data syntax conventions that ensure compatibility with popular data manipulation and visualization libraries in both R and Python.\nInteroperability between tools in the EconDataverse is a key concern, since deficiencies in interoperability create additional toil for the user. In the EconDataverse the problem of interoperability is one of ensuring the tools “speak the same language”. A simple approach to this problem is to consider language as equivalent to syntax, and ensure that tools consume and produce the same syntax, such as the tidy format (TKTK ref) espoused by current EconDataverse tools. This reduces the toil associated with transforming syntax between tools but leaves the problem of semantic interoperability.\nIn this context semantic interoperability is the ability of different tools to exchange data with unambiguous, shared meaning. It ensures that the meaning of the data is preserved and correctly interpreted between tools, even if those tools use different technologies, such as R and Python.\nSemantic interoperability starts from agreeing on unambiguous identifiers for things. For example, the EconDataverse package maintainers have agreed to refer to countries using ISO 3166-1 standard codes and to represent years using four digits and the Gregorian calendar. This consensus ensures that data from different sources about the same countries and years can be joined, as in the R example below:\n\n\n\n\n\n\nNote\n\n\n\nTeal: ChatGPT generated this example. It’d be great to have a better one along the same lines.\n\n\n# Load required package\nlibrary(dplyr)\n\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# --- Perform inner join on ISO country code and year ---\njoined_data &lt;- inner_join(macro_data, population_data, by = c(\"iso3c\", \"year\"))\nImagine that the structure of the two datasets stayed the same but they used different identifiers for countries: one dataset referred to the United States as “États-Unis” while the other referred to it as “USA”.\n# --- Macroeconomic dataset: GDP (in USD billions) ---\nmacro_data &lt;- data.frame(\n  iso3c = c(\"États-Unis\", \"FRA\", \"DEU\", \"JPN\"),\n  year = c(2024, 2024, 2024, 2024),\n  gdp_billion = c(27300, 3100, 4600, 5100)\n)\n\n# --- Population dataset: Population (in millions) ---\npopulation_data &lt;- data.frame(\n  iso3c = c(\"USA\", \"FRA\", \"DEU\", \"BRA\"),\n  year = c(2024, 2024, 2024, 2024),\n  population_million = c(334, 67, 84, 215)\n)\n\n# Join on USA data is not going to work as intended here.\nThe data analyst would have to first resolve the discrepancy – mapping “États-Unis” to “USA” or vice versa – in order to harmonize data from multiple sources. That work is toil that can be eliminated by agreeing on unambiguous identifiers and using them consistently.\n\nUniquely identifying numeric data\nThe R example in figure X also includes numeric data such as GDP (in billions of of US Dollars) and population (in millions). Unlike the ISO 3166 country codes, these data have ambiguous meanings. A human data analyst can infer at least part of the meaning from the column name (gdp_billion) and the accompanying comment (“GDP (in USD billions)”). Other parts are still ambiguous – is it nominal or real GDP?\nWe would like to eliminate this ambiguity and the toil required to resolve it by associating numeric data with unambiguous metadata. The SDMX Information Model recommends that the metadata for a numeric datum include:\n\nDimensions whose combined values uniquely identify that datum in a cube/hypercube of dimensions\nAttributes of the datum, such as units of measurement\n\nThe dimensions uniquely identifying a single nominal GDP measurement are:\n\nPlace: the country associated with the GDP (e.g., “USA” and “FRA”)\nTime: the year the GDP was measured (e.g., 2024).\nIndicator: nominal GDP\n\nDimensions like “place” and attributes like “unit of measurement” should themselves be uniquely identified, and have associated properties such as human-readable labels and definitions, relationships with other dimensions or attributes, and constraints on the values that can be associated with a dimension (e.g., code lists). The SDMX Content-Oriented Guidelines provide a library of well-known, uniquely-identified dimensions and attributes that can be reused in many different domains."
  },
  {
    "objectID": "report.html#linking-data",
    "href": "report.html#linking-data",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Linking data",
    "text": "Linking data\nWe can use the unambiguous identifier for a thing (“USA”) in the data as a key to resolve additional properties about the thing as well as its relationships with other things, following the Linked Data principles. For example, we can use schema.org vocabularies to describe the relationship between the country of the United States and the continent of North America, in a machine-readable format (an RDF knowledge graph represented in Turtle):\n@prefix ex: &lt;http://example.org/entities/&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\n# United States of America\nex:USA a schema:Country ;\n    # Human-readable labels in multiple human languages\n    schema:name \"United States of America\"@en ;\n    schema:name \"États-Unis\"@fr ;\n    schema:identifier \"USA\" ;\n    # A relationship: the country United States is contained in the place North America.\n    schema:containedInPlace ex:NA .\n\n# North America\nex:NA a schema:Continent ;\n    schema:name \"North America\"@en ;\n    schema:identifier \"NA\" ."
  },
  {
    "objectID": "report.html#constraining-data-and-metadata",
    "href": "report.html#constraining-data-and-metadata",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Constraining data and metadata",
    "text": "Constraining data and metadata\nThere is an additional level of consensus at work in the example above. By referencing schema.org we are implicitly agreeing that the unambiguous identifier schema:containedInPlace refers to a way of relating two places. That identifier can be used in turn as a key to further linked metadata:\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;https://schema.org/&gt; .\n\nschema:containedInPlace a rdf:Property ;\n    rdfs:label \"containedInPlace\" ;\n    rdfs:comment \"The basic containment relation between a place and one that contains it.\" ;\n    schema:domainIncludes schema:Place ;\n    schema:inverseOf schema:containsPlace ;\n    schema:rangeIncludes schema:Place .\nThis metadata is part of an ontology. An ontology is an “explicit specification of a conceptualization” (TKTK ref Gruber 1995) of a domain, including the “the types, properties, and interrelationships of entities that exist for a particular domain of discourse” (TKTK ref Gene Ontology Consortium). An ontology includes machine-readable specifications as well as human-readable labels and definitions that codify shared understanding of a domain. In the snippet above the domain is geography – places and a valid relationship (contained-in) between them. The ontology is not about specific places, but about the category “place”, its possible properties, and how it relates to other categories. The example eliminates ambiguity what schema:containedInPlace means through the use of a human-readable definition (“The basic containment relation …”) as well as machine-enforceable constraints on the categories of things that can be involved in the schema:containedInPlace relation (the schema:domainIncludes and schema:rangeIncludes). By constraining the domain of discourse an ontology amplifies the benefits of reusing identifiers by providing further clarity to humans as well as a way for machines to ensure that data and metadata stay within those constraints.\n\nAn ontology of economics\nOntologies like the Financial Industry Business Ontology (FIBO) are commonly used in the financial services sector to enable cross-system federation and aggregation of data in order to support decision-making, streamline regulatory reporting, and encourage the adoption of advanced analytical capabilities.\nUnfortunately, the macroeconomics domain does not have an ontology comparable in scope and rigour to FIBO. We recommend developing an ontology of macroeconomics incrementally. At a minimum this ontology should model:\n\nthe structure of statistical data cubes (dimensions, attributes, measures)\ncommon dimensions such as time, place, and macroeconomic indicators\ncommon attributes such as units\n\nIn the long term the ontology should also model:\n\nrelationships between dimensions e.g., different macroeconomic indicators\nrelationships between attributes e.g., unit conversions\ndata lineage and provenance\nmodeling assumptions\ninformation about economic organizations, people, and the relationships between them (a social network)\ninformation about places and relationships between them (e.g., aggregates such as Low Income Countries)\nmachine-readable assertions extracted from human-written web pages and other documents (a knowledge graph about the economic world)\n\nFortunately, there are a number of existing standards that can serve as building blocks for the ontology, including:\n\nSDMX Data Structure Definitions (DSDs) e.g., Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)\nthe RDF Data Cube Vocabulary for publishing multi-dimensional statistical data as Linked Data\nthe PROV Ontology of data lineage and provenance\nQUDT ontologies of units of measure, quantity kind, dimensions and data types\nSKOS specifications and standards for knowledge organization systems (KOS) (thesauri, taxonomies, et al.)\nthe VoID vocabulary for describing linked datasets\nSchema.org cross-domain vocabularies"
  },
  {
    "objectID": "report.html#tools-to-streamline-economic-data-reuse",
    "href": "report.html#tools-to-streamline-economic-data-reuse",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Tools to streamline economic data reuse",
    "text": "Tools to streamline economic data reuse\nIncreasing semantic interoperability, developing ontologies, and taking advantage of linked data will reduce the toil and increase the productivity of using existing tools in the EconDataverse. However, in order to fully address the challenges highlighted in Section 4, we also recommend expanding and refining the EconDataverse toolkit in three key areas:\n\ntools for extracting and transforming economic data and metadata\ntools for exploring and finding economic data and metadata\ntools for analyzing and manipulating financial models\n\nFigure Figure 1 illustrates our proposal for a new high-level system architecture for EconDataverse tools.\n\n\nWarning: node '7ca3db07800846e2bb1dfd35c8e69468', graph '%3' size too small for label\nWarning: node '8df37a834a9d4f288a641c32a1238792', graph '%3' size too small for label\n\n\n\n\n\n\n\n\nFigure 1: Recommended EconDataverse system architecture\n\n\n\n\n\nGreen lines indicate the flow of ontology-conformant data and metadata while red lines indicate the flow of data in other formats.\nThe following subsections delve into the proposed tool developments in more detail.\n\nTools for extracting, transforming, and loading economic data and metadata\nThe majority of packages in the current EconDataverse package ecosystem extract structured data from IMF, World Bank, and other sources and transform them into tidy format, with each variable as a column and each observation as a row. Metadata are typically limited to column names, which are not explicitly grounded in an ontology that would provide context and definitions. With the notable exception of ISO 3166 country codes, there is relatively little standardization of non-numeric data across data sources.\nIn the short- to mid-term we recommend implementing an adapter library and command-line program that transforms ontology-conformant data to and from the formats expected by existing EconDataverse tools. In the long term these tools should be retrofitted to produce and consume ontology-conformant data and metadata.\nNew tools in the EconDataverse should produce ontology-conformant data and metadata that fully capture the semantics of the source data. These can be easily but lossily transformed into convenient but semantics-poor formats like tidy data frames for compatibility with third party tools.\nExtraction should not be limited to structured data sources. The ontology can also be used to guide Large Language Models (LLMs) in the extraction of ontology-conformant structured data from natural language sources such as the World Bank country and climate development reports or the IMF’s Debt Sustainability Analysis Low-Income Countries (DSA LIC reports). The latter comprise a mixture of narrative text, rendered tables, and charts as PDFs. We recommend starting by extracting statistical data cube-compatible data (dimensions, attributes, measures) from tables in the documents and experimenting with different libraries and APIs such as pdfplumber and AWS Textract to identify a toolchain that maximizes key extraction metrics such as precision, recall, and F1 score.\nOntology-conformant data and metadata extracted and transformed by new and existing tools should be loaded into a store that can preserve the full richness of the data, such as an RDF triple store. The tools described in the next subsection should query this store.\n\n\nTools for exploring and finding economic data and metadata\nTools for exploring and finding data and metadata tend to be useful in proportion to how closely the data’s representation corresponds to the domain abstractions, like accounts and sub-accounts in Balance of Payments datasets. This requires a level of abstraction and interpretation that is difficult to achieve without building data source-specific tools. Instead, most generic tools work with data and metadata at the lowest common denominator of abstraction – as tables and rows of figures to be scanned or a corpus of arbitrary text to be searched. The problem of interpretation is mostly left up to the user.\nHaving data and metadata from multiple sources conform to a rich, domain-specific ontology makes it possible to build tools that work at a higher level of abstraction without sacrificing source independence. Tools can make assumptions about the meaning of the data and relationships between them, which dramatically increases the tools’ power. For example, the next generation of EconDataverse tools could:\n\ngroup or aggregate data from different (geographic) reference areas by considering explicit relationships between areas, such as part-whole or collection membership\nlet users browse and filter cube-structured datasets and slices by the dimensions they incorporate (e.g., time, place, gender), provenance, or recency\nsuggest inputs for a given model by matching metadata about the inputs with metadata from a data/metadata catalog\nsupport federated querying of multiple public and private databases, such as Google Data Commons\n\nThese user-facing tools could have different interfaces or combinations of interfaces: * command line programs * graphical user interfaces * a conversational user interface * programmatic interfaces (APIs), such as a SPARQL endpoint or an SDMX Registry\n\n\nTools for analyzing and manipulating financial models\nFinally, the EconDataverse needs tools for analyzing and financial models, which are typically encoded in Excel spreadsheets. In the short term, tools should automate or semi-automate toilsome tasks like:\n\nManually reverse-engineering spreadsheets to identify which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs\nManually mapping statistical data cube measures to spreadsheet inputs\nManually changing spreadsheets inputs in order to perform sensitivity analyses\n\n\n\nIdentifying minimal spreadsheet inputs\nWe recommend identifying or developing a tool that accepts a spreadsheet-based financial model such as the LIC DSF or the Dynamic Debt Toolkit and identifies which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs. There are two fundamental approaches to this problem:\n\nWhite box: build a dependency graph of spreadsheet formulae spanning the inputs and outputs in order to identify which inputs the desired outputs depend on. This approach is likely to produce false positives – flagging a set of inputs larger than the true minimum – because it is insensitive to the magnitude of input changes.\nBlack box: mutate individual inputs and combinations of inputs and record which outputs change. This approach may produce false negatives – flagging a set of inputs smaller than the true minimum – because of untested interactions between inputs.\n\nIdeally a tool will utilize both approaches to check each other.\n\n\nDisambiguating inputs\nMuch of the toil in using financial models lies in the process of identifying the right source data to supply as model inputs. For example, the Debt Dynamics Toolkit spreadsheet requires the following inputs in a single sheet:\n\nDynamic Debt Toolkit inputs sheet\n\n\n\n\n\n\n\nYear / Variable\n2011\n2012\n\n\n\n\ndt (debt including uncalled guarantees): stock of total gross public debt, percent of GDP\n31.59\n35.34\n\n\no/w stock of local-currency guarantees (uncalled): stock of uncalled guarantees in local currency included in total debt, percent of GDP\n0.00\n0.00\n\n\no/w stock of foreign-currency guarantees (uncalled): stock of uncalled guarantees in foreign currency included in total debt, percent of GDP\n0.00\n0.00\n\n\nαt (share excl. guarantees): share of foreign currency denominated debt in total debt, percent of total debt\n55.05\n58.87\n\n\net (LCU/FCU, avg): nominal average exchange rate, local currency per unit of foreign currency\n18.92\n19.50\n\n\net (LCU/FCU, eop): nominal end of period exchange rate, local currency per unit of foreign currency\n19.05\n19.96\n\n\nitd: nominal effective interest rate on local currency denominated debt, percent\n8.92\n9.23\n\n\nitf: nominal effective interest rate on foreign currency denominated debt, percent\n2.46\n2.03\n\n\nπt: GDP deflator inflation, percent\n5.60\n5.39\n\n\ngt: Real GDP growth, percent\n3.84\n4.13\n\n\npbt: Primary balance, percent of GDP\n-3.28\n-4.03\n\n\noft (other net debt-creating flows): Other net debt creating flows, percent of GDP\n0.00\n0.00\n\n\nπft: Foreign GDP deflator inflation, percent (used in fan chart)\n2.09\n1.92\n\n\n\nPer the data cube model, each input cell can be uniquely identified by dimensions:\n\nTime: with controlled values 2011 and 2012\nPlace: implicitly, the country whose debt is being analyzed\nIndicator: such as “Real GDP growth”\n\nIn this case the year and place are unambiguously identified and are relatively easy to match to source data. It is not as obvious how to map the indicator “nominal effective interest rate on foreign currency denominated debt, percent” to source data.\nOne approach is to treat “nominal effective interest rate on foreign currency denominated debt, percent” as a unique identifier as-is, like Google Data Commons does. This is likely to work for some simple indicators like “nominal GDP” but falters on more complex indicators that could be described in different ways, like this example.\nThe approach we recommend is to decompose the name into a structured combination of sub-identifiers:\n\na base indicator: “interest rate”\nqualifiers on the base indicator: “nominal”, “effective”, “on foreign currency denominated debt”\nattributes: “percent”, which should be treated as an attribute of the data rather than as part of the indicator dimension\n\nThe structure and valid combinations of indicator, qualifier, and attribute values should be dictated by the ontology. Given a permissible set of controlled values, a Large Language Model could aid in this decomposition from natural language.\nThis approach parallels the way World Bank debt codes can be decomposed into segments. For example, the World Bank debt code DT.DOD.[Debt Instrument].[Debtor/Creditor Sector].[Unit] can be decomposed into:\n\nDT: Debt\nDOD: Debt Outstanding and Disbursed\nInstrument Type (e.g., long-term, short-term, use of IMF credit)\nDebtor or Creditor (e.g., public, private, multilateral, bilateral)\nUnit of measure (e.g., CD = current US dollars, GD.ZS = % of GNI)\n\n\n\nAutomating sensitivity analyses with spreadsheets\nThe “black box” approach to identifying minimal spreadsheet inputs, described in Section 5.4.4, can also be used to perform sensitivity analyses by mutating combinations of spreadsheet inputs and recording which outputs change.\n\n\nModels as data, models as code\nIn the long term the financial models themselves should be treated as data, and extracted and transformed into ontology-conformant data and metadata. This approach would build on open source precedents like Morphir, a system that captures an application’s domain model and business logic in a portable / technology-agnostic manner. Given a portable representation of a model and an input dataset, we can execute the model by either:\n\nInterpreting the model representation at runtime.\nGenerating code from the representation and executing the code.\n\nMorphir takes the former approach. We would recommend the latter: lossily generating spreadsheets, Python, R, or other code from ontology types (e.g., dimensions and measures) and a portable model representation. The advantage of this approach is that it would produce an intermediate artifact (generated code) that can be inspected by human users and utilized from non-generated code in a way that ensures model inputs conform to the expected ontology types, above and beyond simple syntax checking."
  },
  {
    "objectID": "report.html#a-tool-for-identifying-minimal-spreadsheet-inputs",
    "href": "report.html#a-tool-for-identifying-minimal-spreadsheet-inputs",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "A tool for identifying minimal spreadsheet inputs",
    "text": "A tool for identifying minimal spreadsheet inputs\nWe recommend identifying or developing a tool that accepts a spreadsheet-based financial model like the Dynamic Debt Toolkit and identifies which inputs are required for a specific subset of desired outputs, in order to avoid spending time populating redundant inputs."
  },
  {
    "objectID": "report.html#a-minimum-ontology-of-economics",
    "href": "report.html#a-minimum-ontology-of-economics",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "A minimum ontology of economics",
    "text": "A minimum ontology of economics\nWe recommend curating a minimum ontology of economics that is sufficient to losslessly capture data and metadata from the selected data sources. The ontology should be specified in SHACL. It should build on the RDF Data Cube Vocabulary as well as existing SDMX Data Structure Definitions such as the Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI).\nFurther, we recommend implementing Python and R libraries for manipulating ontology-conformant data and metadata as type-safe data structures (e.g., classes in Python) and serializing and deserializing data to and from lossless (RDF) and lossy (CSV) formats."
  },
  {
    "objectID": "report.html#extract-transform-load-etl-pipelines",
    "href": "report.html#extract-transform-load-etl-pipelines",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "Extract-transform-load (ETL) pipelines",
    "text": "Extract-transform-load (ETL) pipelines\nWe recommend developing pipelines that extract and transform data and metadata from the two sources and load them into an RDF triple store that enforces ontology conformance using SHACL validation. For batch ETL, we recommend utilizing a workflow orchestration tool such as Apache Airflow or Dagster. The pipelines should use the Python and/or R libraries described in the previous subsection. Each phase of a pipeline (extract, transform, load) should be decoupled from the others in modules that can be reused in other contexts (e.g., doing ad hoc extraction and transformation in a computational notebook). Depending on the data sources, these pipelines could be implemented from scratch to produce ontology-conformant data or they could wrap one of the existing EconDataverse libraries, transforming the existing tidy output to ontology-conformant data. ."
  },
  {
    "objectID": "report.html#sec-recommendations-web-application",
    "href": "report.html#sec-recommendations-web-application",
    "title": "Opportunities to streamline sovereign advisory projects using advanced AI and data technologies",
    "section": "A web application for exploring and finding ontology-conformant data and metadata",
    "text": "A web application for exploring and finding ontology-conformant data and metadata\nWe recommend implementing a full-stack web application for querying the RDF triple store populated by the ETL pipelines. The application should initially consist of a reusable library mediating database access and a graphical user interface that offers multiple forms of search functionality:\n\nfull-text search\nfaceted search\nsemantic search that accepts the string descriptions of inputs in the DDT spreadsheet (c.f., Section 5.4.5) and maps them to well-known data cube dimensions and attributes\n\nThe search functionality of the application should be benchmarked against standard information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG).\nAfter identifying the desired data / metadata, the user should be able to export them from the web application in one of two forms:\n\nlossless RDF\nlossy tidy CSV"
  },
  {
    "objectID": "sections/executive-summary.html",
    "href": "sections/executive-summary.html",
    "title": "Executive summary",
    "section": "",
    "text": "Executive summary"
  },
  {
    "objectID": "sections/problem.html",
    "href": "sections/problem.html",
    "title": "Problem",
    "section": "",
    "text": "Problem\nSovereign debt analysis projects rely heavily on the analysis, processing, and storage of macroeconomic datasets. However, the current data-driven technologies and processes used by Debt Management Offices (DMOs) and sovereign analysts are often outdated, fragmented, and inefficient. These limitations are especially burdensome for resource-constrained institutions that are working to efficiently and effectively enhance debt analysis capacity.\nA significant portion of the work required for sovereign analysis qualifies as toil: manual, repetitive, low-leverage tasks that are often automatable and devoid of long-term value. Toil scales linearly with project complexity, meaning that as the scope of sovereign advisory efforts expands, so too does the operational burden. This introduces significant wasted time and effort into a sovereign analyst’s work and weakens their ability to accomplish high-impact analytical work.\n\nData Processes and Technologies\nSovereign analysts depend on macroeconomic datasets from:\n\nThe International Monetary Fund\nThe World Bank\nOther national and international organizations\n\nHowever, this data is often siloed, difficult to find, hard to access, and/or poorly optimized for reuse. In many cases, the data needed to answer a pressing policy question is technically available but practically inaccessible—buried within PDFs, complex spreadsheets, or outdated web portals.\nThis fragmented data environment complicates fundamental tasks such as harmonizing multiple datasets, preserving data quality, or combining structured and unstructured information. Sovereign analysts may spend hours, if not days, locating, extracting, and validating data before any meaningful analysis can begin. The result is not only wasted time and effort, but also delayed insights that could inform critical policy choices.\n\n\nSpreadsheet-based Models\nFurther compounding the issue is the underdeveloped analytical tools for macroeconomic modeling. Many key workflows still rely on spreadsheet-based financial models, such as the Low-Income Country Debt Sustainability Framework (LIC-DSF) and the Debt Dynamics Tool (DDT).\nThese models are often opaque and complex, with multivariate inputs linked to policy-relevant outputs through undocumented, hard-to-follow logic.\nThere is typically no clear record of how input variables influence key indicators, or how one might efficiently trace through the model to answer a specific policy question.\nThis forces analysts to spend valuable time and effort on reverse-engineering spreadsheets.\nFor example, in the LIC-DSF, the process of identifying the appropriate inputs for variables such as “Indicators of Public and Publicly Guaranteed External Debt under Alternative Scenarios” and “Indicators of Public Debt Under Alternative Scenarios”—both essential for conducting macroeconomic stress tests and assessing a country’s debt-carrying capacity—typically involves:\n\nmanually narrowing down relevant inputs to infer relationships\nmanually validating those inferences\nrecreating the validated logic across new spreadsheets to replicate results\n\nThis process is highly labor-intensive, error-prone, and non-replicable at scale. The absence of automation or documentation not only creates inefficiencies, but also introduces risks to the consistency and credibility of the analysis.\nModern AI technologies and advanced data systems offer an opportunity to reimagine the way sovereign analysis is done. These technologies have the capabilities to address these challenges to enhance the efficiency, accuracy, and impact of sovereign advisory work. By automating repetitive tasks and streamlining data workflows, these tools can eliminate toil and free up a sovereign analyst’s time for more high-value work."
  },
  {
    "objectID": "sections/solution-criteria.html",
    "href": "sections/solution-criteria.html",
    "title": "Solution criteria",
    "section": "",
    "text": "A successful solution should save time and effort on data-driven sovereign advisor projects by eliminating unnecessary toil. In particular, we will focus on opportunities to optimize the use and reuse of data in these projects by making data more findable, accessible, interoperable, and reusable, following the FAIR Guiding Principles (TKTK ref). A solution should address existing challenges in:\n\nfinding and accessing the right economic data and data\nintegrating heterogeneous data from different sources\nexchanging data and metadata between applications and storage in a way that preserves unambiguous, shared meaning\ndealing with missing and low-quality metadata\ntracking data provenance\n\n\n\nA solution should be modular rather than monolithic, with loosely coupled modules that communicate via a well-specified interchange. Each module should “do one thing well”, following the Unix philosophy.\n\n\n\nA solution should be iteratively developed, with frequent releases. Module releases should start from a Minimum Viable Product (MVP) and incrementally add functionality on every release, rather than trying to perfect one feature before moving on to another – the “skateboard to car” approach espoused by Henrik Kniberg.\nThorough testing will be essential to ensuring consistent functionality, protecting against regressions, and helping users trust that releases will work as expected across different environments. Tests should cover the entire test pyramid (also c.f., https://testing.googleblog.com/2010/12/test-sizes.html), with unit tests of individual modules as well as integration tests combining modules.\n\n\n\nA solution should be flexible enough to accommodate unanticipated future sources of data and financial models. It should avoid premature generalization (YAGNI) as well as over-specialization.\n\n\n\nA solution should scale in multiple dimensions, including the number of modules, the number of developers, and the volume of data.\n\n\n\nA solution should be as vendor-neutral as possible. It should build on community-maintained open source projects in preference to commercial packages.\n\n\n\nA solution should avoid needless reinvention and Not Invented Here and take advantage of existing intellectual capital, particularly existing data models.\n\n\n\nNew code should be released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community."
  },
  {
    "objectID": "sections/solution-criteria.html#modularity",
    "href": "sections/solution-criteria.html#modularity",
    "title": "Solution criteria",
    "section": "",
    "text": "A solution should be modular rather than monolithic, with loosely coupled modules that communicate via a well-specified interchange. Each module should “do one thing well”, following the Unix philosophy."
  },
  {
    "objectID": "sections/solution-criteria.html#frequent-releases",
    "href": "sections/solution-criteria.html#frequent-releases",
    "title": "Solution criteria",
    "section": "",
    "text": "A solution should be iteratively developed, with frequent releases. Module releases should start from a Minimum Viable Product (MVP) and incrementally add functionality on every release, rather than trying to perfect one feature before moving on to another – the “skateboard to car” approach espoused by Henrik Kniberg.\nThorough testing will be essential to ensuring consistent functionality, protecting against regressions, and helping users trust that releases will work as expected across different environments. Tests should cover the entire test pyramid (also c.f., https://testing.googleblog.com/2010/12/test-sizes.html), with unit tests of individual modules as well as integration tests combining modules."
  },
  {
    "objectID": "sections/solution-criteria.html#flexibility",
    "href": "sections/solution-criteria.html#flexibility",
    "title": "Solution criteria",
    "section": "",
    "text": "A solution should be flexible enough to accommodate unanticipated future sources of data and financial models. It should avoid premature generalization (YAGNI) as well as over-specialization."
  },
  {
    "objectID": "sections/solution-criteria.html#scalability",
    "href": "sections/solution-criteria.html#scalability",
    "title": "Solution criteria",
    "section": "",
    "text": "A solution should scale in multiple dimensions, including the number of modules, the number of developers, and the volume of data."
  },
  {
    "objectID": "sections/solution-criteria.html#vendor-neutrality",
    "href": "sections/solution-criteria.html#vendor-neutrality",
    "title": "Solution criteria",
    "section": "",
    "text": "A solution should be as vendor-neutral as possible. It should build on community-maintained open source projects in preference to commercial packages."
  },
  {
    "objectID": "sections/solution-criteria.html#reuse",
    "href": "sections/solution-criteria.html#reuse",
    "title": "Solution criteria",
    "section": "",
    "text": "A solution should avoid needless reinvention and Not Invented Here and take advantage of existing intellectual capital, particularly existing data models."
  },
  {
    "objectID": "sections/solution-criteria.html#transparency",
    "href": "sections/solution-criteria.html#transparency",
    "title": "Solution criteria",
    "section": "",
    "text": "New code should be released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community."
  }
]