<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Minor Gordon">
<meta name="author" content="Kweku Ninsin">
<meta name="dcterms.date" content="2025-07-12">

<title>Opportunities to streamline sovereign advisory projects using advanced AI and data technologies</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="report_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap-a48ce70e4257b4e66c396f74f0bf4310.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Opportunities to streamline sovereign advisory projects using advanced AI and data technologies</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Minor Gordon <a href="https://orcid.org/0000-0003-1928-4130" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
             <p>Kweku Ninsin </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2025-07-12</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="executive-summary" class="level1">
<h1>Executive summary</h1>
<!-- Kweku: please draft this once the rest of the report is done. Can use ChatGPT. -->
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Sovereign advisory is the specialized guidance provided to sovereign entities—particularly Ministries of Finance (MOF) and their Debt Management Offices (DMOs)—to strengthen the management of public debt, fiscal policy, and macroeconomic risks. The mission of a sovereign advisor is vital to improving debt sustainability, enhancing access to capital markets, and enabling governments to make informed, data-driven policy decisions that foster long-term economic stability and growth.</p>
<p>Modern technologies offer a powerful pathway forward for transforming sovereign advisory work. Artificial Intelligence (AI) technologies—especially large language models (LLMs)—alongside advanced data systems such as knowledge graphs and ontologies, present a significant opportunity to enhance the capabilities of sovereign analysts. The ability of LLMs to identify and extract information from both structured and unstructured data makes them valuable tools for uncovering insights from complex macroeconomic datasets and policy documents. Knowledge graphs and ontologies structure economic, legal, and financial data in ways that make it more interoperable, findable, and reusable. Together, these technologies can streamline labor-intensive workflows and support more consistent, timely, and data-driven decision-making.</p>
<p>This report explores how AI-powered platforms, when combined with robust data technologies, can unlock new levels of efficiency and effectiveness in sovereign advisory. It provides a detailed analysis of current technologies, workflows, and pain points within the sovereign debt management landscape, identifying key bottlenecks that hinder productivity and insight generation. Building on that analysis, it presents a roadmap for developing tools—along with the associated standards, conventions, and best practices—that will help eliminate toil and enhance the efficiency of both sovereign advisors and resource-constrained DMOs.</p>
<p>Finally, the report offers strategic recommendations for the EconDataverse toolkit. These include long-term foundational technologies and high-level data and software architectures to guide its growth—supporting increased data volume, source heterogeneity, and stakeholder diversity, while maximizing its impact on data-driven sovereign advisory work.</p>
</section>
<section id="problem" class="level1">
<h1>Problem</h1>
<p>Sovereign debt analysis projects rely heavily on the analysis, processing, and storage of macroeconomic datasets. However, the current data-driven technologies and processes used by Debt Management Offices (DMOs) and sovereign analysts are often outdated, fragmented, and inefficient. These limitations are especially burdensome for resource-constrained institutions that are working to efficiently and effectively enhance debt analysis capacity.</p>
<p>A significant portion of the work required for sovereign analysis qualifies as <a href="">toil</a>: manual, repetitive, low-leverage tasks that are often automatable and devoid of long-term value. Toil scales linearly with project complexity, meaning that as the scope of sovereign advisory efforts expands, so too does the operational burden. This introduces significant wasted time and effort into a sovereign analyst’s work and weakens their ability to accomplish high-impact analytical work.</p>
<section id="data-processes-and-technologies" class="level3">
<h3 class="anchored" data-anchor-id="data-processes-and-technologies">Data Processes and Technologies</h3>
<p>Sovereign analysts depend on macroeconomic datasets from:</p>
<ul>
<li><a href="">The International Monetary Fund</a></li>
<li><a href="">The World Bank</a></li>
<li>Other national and international organizations</li>
</ul>
<p>However, this data is often siloed, difficult to find, hard to access, and/or poorly optimized for reuse. In many cases, the data needed to answer a pressing policy question is technically available but practically inaccessible—buried within PDFs, complex spreadsheets, or outdated web portals.</p>
<p>This fragmented data environment complicates fundamental tasks such as harmonizing multiple datasets, preserving data quality, or combining structured and unstructured information. Sovereign analysts may spend hours, if not days, locating, extracting, and validating data before any meaningful analysis can begin. The result is not only wasted time and effort, but also delayed insights that could inform critical policy choices.</p>
</section>
<section id="spreadsheet-based-models" class="level3">
<h3 class="anchored" data-anchor-id="spreadsheet-based-models">Spreadsheet-based Models</h3>
<p>Further compounding the issue is the underdeveloped analytical tools for macroeconomic modeling. Many key workflows still rely on spreadsheet-based financial models, such as the <a href="">Low-Income Country Debt Sustainability Framework (LIC-DSF)</a> and the <a href="">Debt Dynamics Tool (DDT)</a>.</p>
<p>These models are often opaque and complex, with multivariate inputs linked to policy-relevant outputs through undocumented, hard-to-follow logic.</p>
<p>There is typically no clear record of how input variables influence key indicators, or how one might efficiently trace through the model to answer a specific policy question.</p>
<p>This forces analysts to spend valuable time and effort on reverse-engineering spreadsheets.</p>
<p>For example, in the LIC-DSF, the process of identifying the appropriate inputs for variables such as “<strong>Indicators of Public and Publicly Guaranteed External Debt under Alternative Scenarios</strong>” and “<strong>Indicators of Public Debt Under Alternative Scenarios</strong>”—both essential for conducting macroeconomic stress tests and assessing a country’s debt-carrying capacity—typically involves:</p>
<ul>
<li>manually narrowing down relevant inputs to infer relationships</li>
<li>manually validating those inferences</li>
<li>recreating the validated logic across new spreadsheets to replicate results</li>
</ul>
<p>This process is highly labor-intensive, error-prone, and non-replicable at scale. The absence of automation or documentation not only creates inefficiencies, but also introduces risks to the consistency and credibility of the analysis.</p>
<p>Modern AI technologies and advanced data systems offer an opportunity to reimagine the way sovereign analysis is done. These technologies have the capabilities to address these challenges to enhance the efficiency, accuracy, and impact of sovereign advisory work. By automating repetitive tasks and streamlining data workflows, these tools can eliminate toil and free up a sovereign analyst’s time for more high-value work.</p>
</section>
</section>
<section id="sec-solution-criteria" class="level1">
<h1>Solution criteria</h1>
<p>A successful solution should save time and effort on data-driven sovereign advisor projects by eliminating unnecessary toil. In particular, we will focus on opportunities to optimize the use and reuse of data in these projects by making data more <strong>findable</strong>, <strong>accessible</strong>, <strong>interoperable</strong>, and <strong>reusable</strong>, following the FAIR Guiding Principles (TKTK ref). A solution should address existing challenges in:</p>
<ul>
<li>finding and accessing the right economic data and data</li>
<li>integrating heterogeneous data from different sources</li>
<li>exchanging data and metadata between applications and storage in a way that preserves unambiguous, shared meaning</li>
<li>dealing with missing and low-quality metadata</li>
<li>tracking data provenance</li>
</ul>
<section id="modularity" class="level2">
<h2 class="anchored" data-anchor-id="modularity">Modularity</h2>
<p>A solution should be modular rather than monolithic, with <a href="TKTK%20ref">loosely coupled</a> modules that communicate via a well-specified interchange. Each module should “do one thing well”, following the <a href="TKTK%20ref%20Raymond%20Art%20of%20Unix%20Programming">Unix philosophy</a>.</p>
</section>
<section id="frequent-releases" class="level2">
<h2 class="anchored" data-anchor-id="frequent-releases">Frequent releases</h2>
<p>A solution should be iteratively developed, with frequent releases. Module releases should start from a Minimum Viable Product (MVP) and incrementally add functionality on every release, rather than trying to perfect one feature before moving on to another – <a href="https://blog.crisp.se/2016/01/25/henrikkniberg/making-sense-of-mvp">the “skateboard to car” approach espoused by Henrik Kniberg</a>.</p>
<p>Thorough testing will be essential to ensuring consistent functionality, protecting against regressions, and helping users trust that releases will work as expected across different environments. Tests should cover the entire <a href="https://martinfowler.com/articles/practical-test-pyramid.html">test pyramid</a> (also c.f., https://testing.googleblog.com/2010/12/test-sizes.html), with unit tests of individual modules as well as integration tests combining modules.</p>
</section>
<section id="flexibility" class="level2">
<h2 class="anchored" data-anchor-id="flexibility">Flexibility</h2>
<p>A solution should be flexible enough to accommodate unanticipated future sources of data and financial models. It should avoid premature generalization (<a href="TKTK%20ref">YAGNI</a>) as well as over-specialization.</p>
</section>
<section id="scalability" class="level2">
<h2 class="anchored" data-anchor-id="scalability">Scalability</h2>
<p>A solution should scale in multiple dimensions, including the number of modules, the number of developers, and the volume of data.</p>
</section>
<section id="vendor-neutrality" class="level2">
<h2 class="anchored" data-anchor-id="vendor-neutrality">Vendor-neutrality</h2>
<p>A solution should be as vendor-neutral as possible. It should build on community-maintained open source projects in preference to commercial packages.</p>
</section>
<section id="reuse" class="level2">
<h2 class="anchored" data-anchor-id="reuse">Reuse</h2>
<p>A solution should avoid needless reinvention and <a href="TKTK%20ref">Not Invented Here</a> and take advantage of existing intellectual capital, particularly existing data models.</p>
</section>
<section id="transparency" class="level2">
<h2 class="anchored" data-anchor-id="transparency">Transparency</h2>
<p>New code should be released under the MIT License, which permits wide usage and modification. The MIT License promotes transparency, allowing users to freely adapt the tools for their specific needs while contributing improvements to the community.</p>
</section>
</section>
<section id="solution" class="level1">
<h1>Solution</h1>
<p>In the following sections we recommend a solution that entails: * increasing semantic interoperability between tools in the EconDataverse * linking data to other data and metadata * constraining data and metadata using ontologies * developing new tools that rely on semantic interoperability, linked data, and ontologies to streamline economic data reuse</p>
<section id="semantic-interoperability" class="level2">
<h2 class="anchored" data-anchor-id="semantic-interoperability">Semantic interoperability</h2>
<p>The EconDataverse is both a set of software tools in R and Python and a set of conventions and best practices for developing tools that work with economic data. There are coding conventions that ensure consistent style across packages as well as data syntax conventions that ensure compatibility with popular data manipulation and visualization libraries in both R and Python.</p>
<p>Interoperability between tools in the EconDataverse is a key concern, since deficiencies in interoperability create additional toil for the user. In the EconDataverse the problem of interoperability is one of ensuring the tools “speak the same language”. A simple approach to this problem is to consider language as equivalent to syntax, and ensure that tools consume and produce the same syntax, such as the tidy format (TKTK ref) espoused by current EconDataverse tools. This reduces the toil associated with transforming syntax between tools but leaves the problem of <strong>semantic interoperability</strong>.</p>
<p>In this context semantic interoperability is the ability of different tools to exchange data with unambiguous, shared meaning. It ensures that the meaning of the data is preserved and correctly interpreted between tools, even if those tools use different technologies, such as R and Python.</p>
<p>Semantic interoperability starts from agreeing on unambiguous identifiers for things. For example, the EconDataverse package maintainers have agreed to refer to countries using <a href="TKTK%20ref">ISO 3166-1 standard codes</a> and to represent years using four digits and the Gregorian calendar. This consensus ensures that data from different sources about the same countries and years can be joined, as in the R example below:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Teal: ChatGPT generated this example. It’d be great to have a better one along the same lines.</p>
</div>
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load required package</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Macroeconomic dataset: GDP (in USD billions) ---</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>macro_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">iso3c =</span> <span class="fu">c</span>(<span class="st">"USA"</span>, <span class="st">"FRA"</span>, <span class="st">"DEU"</span>, <span class="st">"JPN"</span>),</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">year =</span> <span class="fu">c</span>(<span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">gdp_billion =</span> <span class="fu">c</span>(<span class="dv">27300</span>, <span class="dv">3100</span>, <span class="dv">4600</span>, <span class="dv">5100</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Population dataset: Population (in millions) ---</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>population_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">iso3c =</span> <span class="fu">c</span>(<span class="st">"USA"</span>, <span class="st">"FRA"</span>, <span class="st">"DEU"</span>, <span class="st">"BRA"</span>),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">year =</span> <span class="fu">c</span>(<span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>),</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">population_million =</span> <span class="fu">c</span>(<span class="dv">334</span>, <span class="dv">67</span>, <span class="dv">84</span>, <span class="dv">215</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Perform inner join on ISO country code and year ---</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>joined_data <span class="ot">&lt;-</span> <span class="fu">inner_join</span>(macro_data, population_data, <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"iso3c"</span>, <span class="st">"year"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Imagine that the structure of the two datasets stayed the same but they used different identifiers for countries: one dataset referred to the United States as “États-Unis” while the other referred to it as “USA”.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Macroeconomic dataset: GDP (in USD billions) ---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>macro_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">iso3c =</span> <span class="fu">c</span>(<span class="st">"États-Unis"</span>, <span class="st">"FRA"</span>, <span class="st">"DEU"</span>, <span class="st">"JPN"</span>),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">year =</span> <span class="fu">c</span>(<span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">gdp_billion =</span> <span class="fu">c</span>(<span class="dv">27300</span>, <span class="dv">3100</span>, <span class="dv">4600</span>, <span class="dv">5100</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Population dataset: Population (in millions) ---</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>population_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">iso3c =</span> <span class="fu">c</span>(<span class="st">"USA"</span>, <span class="st">"FRA"</span>, <span class="st">"DEU"</span>, <span class="st">"BRA"</span>),</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">year =</span> <span class="fu">c</span>(<span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>, <span class="dv">2024</span>),</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">population_million =</span> <span class="fu">c</span>(<span class="dv">334</span>, <span class="dv">67</span>, <span class="dv">84</span>, <span class="dv">215</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Join on USA data is not going to work as intended here.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The data analyst would have to first resolve the discrepancy – mapping “États-Unis” to “USA” or vice versa – in order to harmonize data from multiple sources. That work is toil that can be eliminated by agreeing on unambiguous identifiers and using them consistently.</p>
<section id="uniquely-identifying-numeric-data" class="level3">
<h3 class="anchored" data-anchor-id="uniquely-identifying-numeric-data">Uniquely identifying numeric data</h3>
<p>The R example in <a href="TKTK%20ref">figure X</a> also includes numeric data such as GDP (in billions of of US Dollars) and population (in millions). Unlike the ISO 3166 country codes, these data have ambiguous meanings. A human data analyst can infer at least part of the meaning from the column name (gdp_billion) and the accompanying comment (“GDP (in USD billions)”). Other parts are still ambiguous – is it nominal or real GDP?</p>
<p>We would like to eliminate this ambiguity and the toil required to resolve it by associating numeric data with unambiguous metadata. The <a href="TKTK%20ref">SDMX Information Model</a> recommends that the metadata for a numeric datum include:</p>
<ul>
<li><a href="TKTK%20ref%20to%20SDMX%20Information%20Model">Dimensions</a> whose combined values uniquely identify that datum in a <a href="TKTK%20ref%20OLAP%20Cube">cube/hypercube of dimensions</a></li>
<li><a href="TKTK%20ref%20to%20SDMX%20information%20Model">Attributes</a> of the datum, such as units of measurement</li>
</ul>
<p>The dimensions uniquely identifying a single nominal GDP measurement are:</p>
<ol type="1">
<li>Place: the country associated with the GDP (e.g., “USA” and “FRA”)</li>
<li>Time: the year the GDP was measured (e.g., 2024).</li>
<li>Indicator: nominal GDP</li>
</ol>
<p>Dimensions like “place” and attributes like “unit of measurement” should themselves be uniquely identified, and have associated properties such as human-readable labels and definitions, relationships with other dimensions or attributes, and constraints on the values that can be associated with a dimension (e.g., <a href="TKTK%20ref">code lists</a>). The <a href="TKTK%20ref">SDMX Content-Oriented Guidelines</a> provide a library of well-known, uniquely-identified dimensions and attributes that can be reused in many different domains.</p>
</section>
</section>
<section id="linking-data" class="level2">
<h2 class="anchored" data-anchor-id="linking-data">Linking data</h2>
<p>We can use the unambiguous identifier for a thing (“USA”) in the data as a key to resolve additional properties about the thing as well as its relationships with other things, following the <a href="http://linkeddatabook.com/editions/1.0/">Linked Data principles</a>. For example, we can use <a href="https://schema.org">schema.org</a> vocabularies to describe the relationship between the country of the United States and the continent of North America, in a machine-readable format (an <a href="TKTK%20ref">RDF</a> <a href="TKTK%20ref">knowledge graph</a> represented in <a href="TKTK%20ref">Turtle</a>):</p>
<pre class="ttl"><code>@prefix ex: &lt;http://example.org/entities/&gt; .
@prefix schema: &lt;https://schema.org/&gt; .

# United States of America
ex:USA a schema:Country ;
    # Human-readable labels in multiple human languages
    schema:name "United States of America"@en ;
    schema:name "États-Unis"@fr ;
    schema:identifier "USA" ;
    # A relationship: the country United States is contained in the place North America.
    schema:containedInPlace ex:NA .

# North America
ex:NA a schema:Continent ;
    schema:name "North America"@en ;
    schema:identifier "NA" .</code></pre>
</section>
<section id="constraining-data-and-metadata" class="level2">
<h2 class="anchored" data-anchor-id="constraining-data-and-metadata">Constraining data and metadata</h2>
<p>There is an additional level of consensus at work in the example above. By referencing schema.org we are implicitly agreeing that the unambiguous identifier <code>schema:containedInPlace</code> refers to a way of relating two places. That identifier can be used in turn as a key to further linked metadata:</p>
<pre class="ttl"><code>@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix schema: &lt;https://schema.org/&gt; .

schema:containedInPlace a rdf:Property ;
    rdfs:label "containedInPlace" ;
    rdfs:comment "The basic containment relation between a place and one that contains it." ;
    schema:domainIncludes schema:Place ;
    schema:inverseOf schema:containsPlace ;
    schema:rangeIncludes schema:Place .</code></pre>
<p>This metadata is part of an <strong>ontology</strong>. An ontology is an “explicit specification of a conceptualization” (TKTK ref Gruber 1995) of a domain, including the “the types, properties, and interrelationships of entities that exist for a particular domain of discourse” (TKTK ref Gene Ontology Consortium). An ontology includes machine-readable specifications as well as human-readable labels and definitions that codify shared understanding of a domain. In the snippet above the domain is geography – places and a valid relationship (contained-in) between them. The ontology is not about specific places, but about the category “place”, its possible properties, and how it relates to other categories. The example eliminates ambiguity what <code>schema:containedInPlace</code> means through the use of a human-readable definition (“The basic containment relation …”) as well as machine-enforceable <strong>constraints</strong> on the categories of things that can be involved in the <code>schema:containedInPlace</code> relation (the <code>schema:domainIncludes</code> and <code>schema:rangeIncludes</code>). By constraining the domain of discourse an ontology amplifies the benefits of reusing identifiers by providing further clarity to humans as well as a way for machines to ensure that data and metadata stay within those constraints.</p>
<section id="an-ontology-of-economics" class="level3">
<h3 class="anchored" data-anchor-id="an-ontology-of-economics">An ontology of economics</h3>
<p>Ontologies like the <a href="https://spec.edmcouncil.org/fibo/">Financial Industry Business Ontology (FIBO)</a> are commonly used in the financial services sector to enable cross-system federation and aggregation of data in order to support decision-making, streamline regulatory reporting, and encourage the adoption of advanced analytical capabilities.</p>
<p>Unfortunately, the macroeconomics domain does not have an ontology comparable in scope and rigour to FIBO. We recommend developing an ontology of macroeconomics incrementally. At a minimum this ontology should model:</p>
<ul>
<li>the structure of statistical data cubes (dimensions, attributes, measures)</li>
<li>common dimensions such as time, place, and macroeconomic indicators</li>
<li>common attributes such as units</li>
</ul>
<p>In the long term the ontology should also model:</p>
<ul>
<li>relationships between dimensions e.g., different macroeconomic indicators</li>
<li>relationships between attributes e.g., unit conversions</li>
<li>data lineage and provenance</li>
<li>modeling assumptions</li>
<li>information about economic organizations, people, and the relationships between them (a <a href="TKTK%20ref">social network</a>)</li>
<li>information about places and relationships between them (e.g., aggregates such as Low Income Countries)</li>
<li>machine-readable assertions extracted from human-written web pages and other documents (a <a href="TKTK%20ref">knowledge graph</a> about the economic world)</li>
</ul>
<p>Fortunately, there are a number of existing standards that can serve as building blocks for the ontology, including:</p>
<ul>
<li><a href="TKTK%20ref">SDMX Data Structure Definitions (DSDs)</a> e.g., <a href="https://sdmx.org/sdmx-data-structure-definitions-for-balance-of-payments-sdmx-bop/">Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)</a></li>
<li>the <a href="https://www.w3.org/TR/vocab-data-cube/">RDF Data Cube Vocabulary</a> for publishing multi-dimensional statistical data as <a href="TKTK%20ref">Linked Data</a></li>
<li>the <a href="https://www.w3.org/TR/prov-o/">PROV Ontology</a> of data lineage and provenance</li>
<li><a href="https://qudt.org">QUDT</a> ontologies of units of measure, quantity kind, dimensions and data types</li>
<li><a href="https://www.w3.org/2004/02/skos/">SKOS</a> specifications and standards for knowledge organization systems (KOS) (thesauri, taxonomies, et al.)</li>
<li>the <a href="https://www.w3.org/TR/void/">VoID vocabulary</a> for describing linked datasets</li>
<li><a href="https://schema.org">Schema.org</a> cross-domain vocabularies</li>
</ul>
</section>
</section>
<section id="tools-to-streamline-economic-data-reuse" class="level2">
<h2 class="anchored" data-anchor-id="tools-to-streamline-economic-data-reuse">Tools to streamline economic data reuse</h2>
<p>Increasing semantic interoperability, developing ontologies, and taking advantage of linked data will reduce the toil and increase the productivity of using existing tools in the EconDataverse. However, in order to fully address the challenges highlighted in <a href="#sec-solution-criteria" class="quarto-xref">Section&nbsp;4</a>, we also recommend expanding and refining the EconDataverse toolkit in three key areas:</p>
<ul>
<li>tools for extracting and transforming economic data and metadata</li>
<li>tools for exploring and finding economic data and metadata</li>
<li>tools for analyzing and manipulating financial models</li>
</ul>
<p>Figure <a href="#fig-solution-diagram" class="quarto-xref">Figure&nbsp;1</a> illustrates our proposal for a new high-level system architecture for EconDataverse tools.</p>
<div id="cell-fig-solution-diagram" class="cell" data-execution_count="1">
<div class="cell-output cell-output-stderr">
<pre><code>Warning: node 'e3459125a11549f0be5f799e4db160f8', graph '%3' size too small for label
Warning: node 'b9b929f7272e4815917293cd6a4c039f', graph '%3' size too small for label
Warning: node '4e4600f68fee4f639149f31d9beb5c86', graph '%3' size too small for label</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div id="fig-solution-diagram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-solution-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="report_files/figure-html/fig-solution-diagram-output-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-solution-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Recommended EconDataverse system architecture
</figcaption>
</figure>
</div>
</div>
</div>
<p>Green lines indicate the flow of ontology-conformant data and metadata while red lines indicate the flow of data in other formats.</p>
<p>The following subsections delve into the proposed tool developments in more detail.</p>
<section id="sec-solution-etl" class="level3">
<h3 class="anchored" data-anchor-id="sec-solution-etl">Tools for extracting, transforming, and loading economic data and metadata</h3>
<p>The majority of packages in the current EconDataverse package ecosystem extract structured data from IMF, World Bank, and other sources and transform them into <a href="TKTK%20ref">tidy format</a>, with each variable as a column and each observation as a row. Metadata are typically limited to column names, which are not explicitly grounded in an ontology that would provide context and definitions. With the notable exception of ISO 3166 country codes, there is relatively little standardization of non-numeric data across data sources.</p>
<p>In the short- to mid-term we recommend implementing an adapter library and command-line program that transforms ontology-conformant data to and from the formats expected by existing EconDataverse tools. In the long term these tools should be retrofitted to produce and consume ontology-conformant data and metadata.</p>
<p>New tools in the EconDataverse should produce ontology-conformant data and metadata that fully capture the semantics of the source data. These can be easily but lossily transformed into convenient but semantics-poor formats like tidy data frames for compatibility with third party tools.</p>
<p>Extraction should not be limited to structured data sources. The ontology can also be used to guide Large Language Models (LLMs) in the extraction of ontology-conformant structured data from natural language sources such as the <a href="TKTK%20ref">World Bank country and climate development reports</a> or the IMF’s <a href="https://www.imf.org/en/Publications/DSA">Debt Sustainability Analysis Low-Income Countries (DSA LIC reports)</a>. The latter comprise a mixture of narrative text, rendered tables, and charts as PDFs. We recommend starting by extracting statistical data cube-compatible data (dimensions, attributes, measures) from tables in the documents and experimenting with different libraries and APIs such as <a href="TKTK">pdfplumber</a> and <a href="TKTK">AWS Textract</a> to identify a toolchain that maximizes key extraction metrics such as <a href="TKTK%20ref">precision, recall, and F1 score</a>.</p>
<p>Ontology-conformant data and metadata extracted and transformed by new and existing tools should be loaded into a store that can preserve the full richness of the data, such as an <a href="TKTK%20ref">RDF triple store</a>. The tools described in the next subsection should query this store.</p>
</section>
<section id="sec-solution-ui" class="level3">
<h3 class="anchored" data-anchor-id="sec-solution-ui">Tools for exploring and finding economic data and metadata</h3>
<p>Tools for exploring and finding data and metadata tend to be useful in proportion to how closely the data’s representation corresponds to the domain abstractions, like accounts and sub-accounts in Balance of Payments datasets. This requires a level of abstraction and interpretation that is difficult to achieve without building data source-specific tools. Instead, most generic tools work with data and metadata at the lowest common denominator of abstraction – as tables and rows of figures to be scanned or a corpus of arbitrary text to be searched. The problem of interpretation is mostly left up to the user.</p>
<p>Having data and metadata from multiple sources conform to a rich, domain-specific ontology makes it possible to build tools that work at a higher level of abstraction without sacrificing source independence. Tools can make assumptions about the meaning of the data and relationships between them, which dramatically increases the tools’ power. For example, the next generation of EconDataverse tools could:</p>
<ul>
<li>group or aggregate data from different (geographic) reference areas by considering explicit relationships between areas, such as part-whole or collection membership</li>
<li>let users browse and filter cube-structured datasets and slices by the dimensions they incorporate (e.g., time, place, gender), provenance, or recency</li>
<li>suggest inputs for a given model by matching metadata about the inputs with metadata from a data/metadata catalog</li>
<li>support <a href="https://ruben.verborgh.org/blog/2015/06/09/federated-sparql-queries-in-your-browser/">federated querying</a> of multiple public and private databases, such as <a href="TKTK%20ref">Google Data Commons</a></li>
</ul>
<p>These user-facing tools could have different interfaces or combinations of interfaces: * command line programs * graphical user interfaces * a <a href="https://en.wikipedia.org/wiki/Conversational_user_interface">conversational user interface</a> * programmatic interfaces (APIs), such as a <a href="TKTK%20ref">SPARQL endpoint</a> or <a href="TKTK%20ref">an SDMX Registry</a></p>
</section>
<section id="tools-for-analyzing-and-manipulating-financial-models" class="level3">
<h3 class="anchored" data-anchor-id="tools-for-analyzing-and-manipulating-financial-models">Tools for analyzing and manipulating financial models</h3>
<p>Finally, the EconDataverse needs tools for analyzing and financial models, which are typically encoded in Excel spreadsheets. In the short term, tools should automate or semi-automate toilsome tasks like:</p>
<ul>
<li>Manually reverse-engineering spreadsheets to identify which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs</li>
<li>Manually mapping statistical data cube measures to spreadsheet inputs</li>
<li>Manually changing spreadsheets inputs in order to perform sensitivity analyses</li>
</ul>
</section>
<section id="sec-solution-identifying-minimal-spreadsheet-inputs" class="level3">
<h3 class="anchored" data-anchor-id="sec-solution-identifying-minimal-spreadsheet-inputs">Identifying minimal spreadsheet inputs</h3>
<p>We recommend identifying or developing a tool that accepts a spreadsheet-based financial model such as the <a href="TKTK%20ref">LIC DSF</a> or the <a href="TKTK%20ref">Dynamic Debt Toolkit</a> and identifies which inputs are required for a subset of desired outputs, in order to avoid spending time populating redundant inputs. There are two fundamental approaches to this problem:</p>
<ul>
<li><strong>White box</strong>: build a <a href="https://en.wikipedia.org/wiki/Dependency_graph">dependency graph</a> of spreadsheet formulae spanning the inputs and outputs in order to identify which inputs the desired outputs depend on. This approach is likely to produce false positives – flagging a set of inputs larger than the true minimum – because it is insensitive to the magnitude of input changes.</li>
<li><strong>Black box</strong>: mutate individual inputs and combinations of inputs and record which outputs change. This approach may produce false negatives – flagging a set of inputs smaller than the true minimum – because of untested interactions between inputs.</li>
</ul>
<p>Ideally a tool will utilize both approaches to check each other.</p>
</section>
<section id="sec-solution-disambiguating-inputs" class="level3">
<h3 class="anchored" data-anchor-id="sec-solution-disambiguating-inputs">Disambiguating inputs</h3>
<p>Much of the toil in using financial models lies in the process of identifying the right source data to supply as model inputs. For example, the Debt Dynamics Toolkit spreadsheet requires the following inputs in a single sheet:</p>
<table class="table-bordered table-striped table-hover caption-top table">
<caption>Dynamic Debt Toolkit inputs sheet</caption>
<colgroup>
<col style="width: 53%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Year / Variable</th>
<th>2011</th>
<th>2012</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dt (debt including uncalled guarantees): stock of total gross public debt, percent of GDP</td>
<td>31.59</td>
<td>35.34</td>
</tr>
<tr class="even">
<td>o/w stock of local-currency guarantees (uncalled): stock of uncalled guarantees in local currency included in total debt, percent of GDP</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr class="odd">
<td>o/w stock of foreign-currency guarantees (uncalled): stock of uncalled guarantees in foreign currency included in total debt, percent of GDP</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr class="even">
<td>αt (share excl. guarantees): share of foreign currency denominated debt in total debt, percent of total debt</td>
<td>55.05</td>
<td>58.87</td>
</tr>
<tr class="odd">
<td>et (LCU/FCU, avg): nominal average exchange rate, local currency per unit of foreign currency</td>
<td>18.92</td>
<td>19.50</td>
</tr>
<tr class="even">
<td>et (LCU/FCU, eop): nominal end of period exchange rate, local currency per unit of foreign currency</td>
<td>19.05</td>
<td>19.96</td>
</tr>
<tr class="odd">
<td>itd: nominal effective interest rate on local currency denominated debt, percent</td>
<td>8.92</td>
<td>9.23</td>
</tr>
<tr class="even">
<td>itf: nominal effective interest rate on foreign currency denominated debt, percent</td>
<td>2.46</td>
<td>2.03</td>
</tr>
<tr class="odd">
<td>πt: GDP deflator inflation, percent</td>
<td>5.60</td>
<td>5.39</td>
</tr>
<tr class="even">
<td>gt: Real GDP growth, percent</td>
<td>3.84</td>
<td>4.13</td>
</tr>
<tr class="odd">
<td>pbt: Primary balance, percent of GDP</td>
<td>-3.28</td>
<td>-4.03</td>
</tr>
<tr class="even">
<td>oft (other net debt-creating flows): Other net debt creating flows, percent of GDP</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr class="odd">
<td>πft: Foreign GDP deflator inflation, percent (used in fan chart)</td>
<td>2.09</td>
<td>1.92</td>
</tr>
</tbody>
</table>
<p>Per the <a href="TKTK%20ref">data cube model</a>, each input cell can be uniquely identified by dimensions:</p>
<ul>
<li>Time: with controlled values 2011 and 2012</li>
<li>Place: implicitly, the country whose debt is being analyzed</li>
<li>Indicator: such as “Real GDP growth”</li>
</ul>
<p>In this case the year and place are unambiguously identified and are relatively easy to match to source data. It is not as obvious how to map the indicator “nominal effective interest rate on foreign currency denominated debt, percent” to source data.</p>
<p>One approach is to treat “nominal effective interest rate on foreign currency denominated debt, percent” as a unique identifier as-is, like <a href="TKTK%20ref">Google Data Commons</a> does. This is likely to work for some simple indicators like “nominal GDP” but falters on more complex indicators that could be described in different ways, like this example.</p>
<p>The approach we recommend is to decompose the name into a structured combination of sub-identifiers:</p>
<ul>
<li>a base indicator: “interest rate”</li>
<li>qualifiers on the base indicator: “nominal”, “effective”, “on foreign currency denominated debt”</li>
<li>attributes: “percent”, which should be treated as an attribute of the data rather than as part of the indicator dimension</li>
</ul>
<p>The structure and valid combinations of indicator, qualifier, and attribute values should be dictated by the ontology. Given a permissible set of controlled values, a Large Language Model could aid in this decomposition from natural language.</p>
<p>This approach parallels the way World Bank debt codes can be decomposed into segments. For example, the World Bank debt code <code>DT.DOD.[Debt Instrument].[Debtor/Creditor Sector].[Unit]</code> can be decomposed into:</p>
<ul>
<li>DT: Debt</li>
<li>DOD: Debt Outstanding and Disbursed</li>
<li>Instrument Type (e.g., long-term, short-term, use of IMF credit)</li>
<li>Debtor or Creditor (e.g., public, private, multilateral, bilateral)</li>
<li>Unit of measure (e.g., CD = current US dollars, GD.ZS = % of GNI)</li>
</ul>
</section>
<section id="automating-sensitivity-analyses-with-spreadsheets" class="level3">
<h3 class="anchored" data-anchor-id="automating-sensitivity-analyses-with-spreadsheets">Automating sensitivity analyses with spreadsheets</h3>
<p>The “black box” approach to identifying minimal spreadsheet inputs, described in <a href="#sec-solution-identifying-minimal-spreadsheet-inputs" class="quarto-xref">Section&nbsp;5.4.4</a>, can also be used to perform sensitivity analyses by mutating combinations of spreadsheet inputs and recording which outputs change.</p>
</section>
<section id="models-as-data-models-as-code" class="level3">
<h3 class="anchored" data-anchor-id="models-as-data-models-as-code">Models as data, models as code</h3>
<p>In the long term the financial models themselves should be treated as data, and extracted and transformed into ontology-conformant data and metadata. This approach would build on open source precedents like <a href="https://morphir.finos.org/">Morphir</a>, a system that captures an application’s domain model and business logic in a portable / technology-agnostic manner. Given a portable representation of a model and an input dataset, we can execute the model by either:</p>
<ol type="1">
<li><a href="https://en.wikipedia.org/wiki/Interpreter_(computing)">Interpreting</a> the model representation at runtime.</li>
<li><a href="https://en.wikipedia.org/wiki/Code_generation_(compiler)">Generating code</a> from the representation and executing the code.</li>
</ol>
<p>Morphir takes the former approach. We would recommend the latter: lossily generating spreadsheets, Python, R, or other code from ontology types (e.g., dimensions and measures) and a portable model representation. The advantage of this approach is that it would produce an intermediate artifact (generated code) that can be inspected by human users and utilized from non-generated code in a way that ensures model inputs conform to the expected ontology types, above and beyond simple syntax checking.</p>
</section>
</section>
</section>
<section id="recommendations" class="level1">
<h1>Recommendations</h1>
<p>We recommend implementing the proposed EconDataverse system architecture (<a href="#fig-solution-diagram" class="quarto-xref">Figure&nbsp;1</a>) iteratively, following <a href="https://agilemanifesto.org/principles.html">agile principles</a>. Rather than focusing on any single part of the system architecture, we recommend developing a set of minimum viable implementations of the different components that together address a useful, end-to-end scenario involving a simple economic model, the IMF’s <a href="TKTK%20ref">Dynamic Debt Toolkit (DDT)</a>, applied to a reference country (Ghana). Specifically, the tools should help the human analyst:</p>
<ol type="1">
<li>Identify a minimum set of spreadsheet inputs for a given output of the DDT template, as described in <a href="#sec-solution-identifying-minimal-spreadsheet-inputs" class="quarto-xref">Section&nbsp;5.4.4</a></li>
<li>Map the input labels to known data cube dimensions and attributes, as described in <a href="#sec-solution-disambiguating-inputs" class="quarto-xref">Section&nbsp;5.4.5</a></li>
<li>Find data associated with these dimensions in a store of ontology-conformant data and metadata, as described in <a href="#sec-solution-ui" class="quarto-xref">Section&nbsp;5.4.2</a></li>
</ol>
<p>A batch process should eagerly populate the store queried by step #3 by extracting, transforming, and loading a subset of relevant ontology-conformant data and metadata (per <a href="#sec-solution-etl" class="quarto-xref">Section&nbsp;5.4.1</a>) from at least two data sources, such as</p>
<ol type="1">
<li><a href="https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation">the World Bank’s World Development Indicators (WDI) API</a></li>
<li><a href="TKTK%20ref">the IMF’s World Economic Outlook (WEO) database</a></li>
</ol>
<p>The aggregation, linking, and subsequent querying of data and metadata from multiple sources will demonstrate the value of ontology-based semantic interoperability between tools.</p>
<p>The following subsections recommend specific implementation projects to address the end-to-end scenario.</p>
<section id="a-tool-for-identifying-minimal-spreadsheet-inputs" class="level2">
<h2 class="anchored" data-anchor-id="a-tool-for-identifying-minimal-spreadsheet-inputs">A tool for identifying minimal spreadsheet inputs</h2>
<p>We recommend identifying or developing a tool that accepts a spreadsheet-based financial model like the <a href="TKTK">Dynamic Debt Toolkit</a> and identifies which inputs are required for a specific subset of desired outputs, in order to avoid spending time populating redundant inputs.</p>
</section>
<section id="a-minimum-ontology-of-economics" class="level2">
<h2 class="anchored" data-anchor-id="a-minimum-ontology-of-economics">A minimum ontology of economics</h2>
<p>We recommend curating a minimum ontology of economics that is sufficient to losslessly capture data and metadata from the selected data sources. The ontology should be specified in <a href="TKTK%20ref">SHACL</a>. It should build on the <a href="TKTK%20ref">RDF Data Cube Vocabulary</a> as well as existing <a href="TKTK%20ref">SDMX Data Structure Definitions</a> such as the <a href="https://sdmx.org/sdmx-data-structure-definitions-for-balance-of-payments-sdmx-bop/">Balance of Payments (SDMX-BOP and Foreign Debt Investments (SDMX-FDI)</a>.</p>
<p>Further, we recommend implementing Python and R libraries for manipulating ontology-conformant data and metadata as type-safe data structures (e.g., classes in Python) and serializing and deserializing data to and from lossless (RDF) and lossy (CSV) formats.</p>
</section>
<section id="extract-transform-load-etl-pipelines" class="level2">
<h2 class="anchored" data-anchor-id="extract-transform-load-etl-pipelines">Extract-transform-load (ETL) pipelines</h2>
<p>We recommend developing pipelines that extract and transform data and metadata from the two sources and load them into an RDF triple store that enforces ontology conformance using <a href="TKTK%20ref">SHACL validation</a>. For batch ETL, we recommend utilizing a workflow orchestration tool such as <a href="TKTK%20ref">Apache Airflow</a> or <a href="TKTK%20ref">Dagster</a>. The pipelines should use the Python and/or R libraries described in the previous subsection. Each phase of a pipeline (extract, transform, load) should be decoupled from the others in modules that can be reused in other contexts (e.g., doing ad hoc extraction and transformation in a computational notebook). Depending on the data sources, these pipelines could be implemented from scratch to produce ontology-conformant data or they could wrap one of the existing EconDataverse libraries, transforming the existing tidy output to ontology-conformant data. .</p>
</section>
<section id="sec-recommendations-web-application" class="level2">
<h2 class="anchored" data-anchor-id="sec-recommendations-web-application">A web application for exploring and finding ontology-conformant data and metadata</h2>
<p>We recommend implementing a full-stack web application for querying the RDF triple store populated by the ETL pipelines. The application should initially consist of a reusable library mediating database access and a graphical user interface that offers multiple forms of search functionality:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Full-text_search">full-text search</a></li>
<li><a href="https://en.wikipedia.org/wiki/Faceted_search">faceted search</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semantic_search">semantic search</a> that accepts the string descriptions of inputs in the DDT spreadsheet (c.f., <a href="#sec-solution-disambiguating-inputs" class="quarto-xref">Section&nbsp;5.4.5</a>) and maps them to well-known data cube dimensions and attributes</li>
</ul>
<p>The search functionality of the application should be benchmarked against standard information retrieval metrics such as <a href="TKTK%20ref">Normalized Discounted Cumulative Gain (NDCG)</a>.</p>
<p>After identifying the desired data / metadata, the user should be able to export them from the web application in one of two forms:</p>
<ul>
<li>lossless RDF</li>
<li>lossy <a href="TKTK%20ref">tidy CSV</a></li>
</ul>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<!-- can restate the most important points from the white paper overall, or it can restate the case for addressing the problem. -->
<div class="refs">

</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>